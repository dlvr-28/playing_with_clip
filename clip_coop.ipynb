{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U torch torchvision timm open_clip_torch\n",
    "\n",
    "import torch, open_clip\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Choose a model from recent CLIP-family work ===\n",
    "# Classic strong baseline:\n",
    "MODEL_NAME   = \"ViT-B-32\"\n",
    "PRETRAINED   = \"laion2b_s34b_b79k\"   # from OpenCLIP\n",
    "\n",
    "# Tip: try very recent ones too (if available in your env):\n",
    "# MODEL_NAME, PRETRAINED = \"ViT-SO400M-14-SigLIP\", \"webli\"        # SigLIP family\n",
    "# MODEL_NAME, PRETRAINED = \"EVA02-L-14\", \"laion2b_s9b_b144k\"      # EVA-CLIP family\n",
    "# (List available combos:)\n",
    "# import pprint; pprint.pp(open_clip.list_pretrained())\n",
    "\n",
    "# --- Load model + preprocess ---\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# --- Zero-shot on CIFAR-10 (tiny & quick) ---\n",
    "cifar = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=preprocess)\n",
    "loader = DataLoader(cifar, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "classnames = cifar.classes\n",
    "\n",
    "def get_text_features(classnames):\n",
    "    prompts = [f\"a photo of a {c}\" for c in classnames]\n",
    "    with torch.no_grad():\n",
    "        text_tokens   = tokenizer(prompts).to(device)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "text_features = get_text_features(classnames)\n",
    "\n",
    "def test_model(model, text_features, test_loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            # CLIP-style scaled cosine sims\n",
    "            logits = 100.0 * image_features @ text_features.T\n",
    "            preds = logits.argmax(dim=-1).cpu()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Zero-shot accuracy: {100*accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "test_model(model, text_features, loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Few-shot split (16-shot/class) -----------------------------------------\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "import random, math, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "# CIFAR-10 train set for few-shot\n",
    "train_tf = preprocess  # you can make it stronger later; using same as test keeps things simple\n",
    "cifar_train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "\n",
    "def build_fewshot_indices(dataset, k_per_class=16):\n",
    "    idxs_by_c = {c: [] for c in range(10)}\n",
    "    for i, (_, y) in enumerate(dataset):\n",
    "        if len(idxs_by_c[y]) < k_per_class:\n",
    "            idxs_by_c[y].append(i)\n",
    "        if all(len(v) >= k_per_class for v in idxs_by_c.values()):\n",
    "            break\n",
    "    fewshot = [i for c in range(10) for i in idxs_by_c[c]]\n",
    "    return fewshot\n",
    "\n",
    "few_idx = build_fewshot_indices(cifar_train, k_per_class=16)\n",
    "fewshot_loader = DataLoader(Subset(cifar_train, few_idx), batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# --- CoOp: learnable prompt context -----------------------------------------\n",
    "@torch.no_grad()\n",
    "def tokenize_classnames(classnames, ctx_prefix=\"\", ctx_suffix=\"\"):\n",
    "    # We'll still need tokenized class name pieces for positions after context\n",
    "    texts = [f\"{ctx_prefix}{name}{ctx_suffix}\" for name in classnames]\n",
    "    return tokenizer(texts)\n",
    "\n",
    "class SimpleCoOp(nn.Module):\n",
    "    \"\"\"\n",
    "    Learn n_ctx context tokens as free embeddings; keep CLIP frozen.\n",
    "    \"\"\"\n",
    "    def __init__(self, clip_model, classnames, n_ctx=8):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        for p in self.clip.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.classnames = classnames\n",
    "        self.dtype = next(self.clip.parameters()).dtype\n",
    "        self.ctx_len = n_ctx\n",
    "\n",
    "        # CLIP text parts we need\n",
    "        self.token_embedding = self.clip.token_embedding           # (vocab, dim)\n",
    "        self.positional_embedding = self.clip.positional_embedding  # (n_ctx, dim)\n",
    "        self.transformer = self.clip.transformer\n",
    "        self.ln_final = self.clip.ln_final\n",
    "        self.text_projection = self.clip.text_projection\n",
    "        self.register_buffer(\"attn_mask\", self.clip.attn_mask, persistent=False)\n",
    "\n",
    "        # init context vectors (learnable)\n",
    "        ctx_dim = self.token_embedding.weight.shape[1]\n",
    "        self.ctx = nn.Parameter(torch.randn(self.ctx_len, ctx_dim) * 0.02)\n",
    "\n",
    "        # tokenized classnames to get their tokens/embs (frozen)\n",
    "        self.classname_tokens = tokenize_classnames(classnames).to(self.ctx.device if self.ctx.is_cuda else \"cpu\")\n",
    "\n",
    "        # end-of-text token index\n",
    "        try:\n",
    "            from open_clip.tokenizer import _tokenizer\n",
    "            self.eot_token = _tokenizer.eot_token\n",
    "        except Exception:\n",
    "            # fallback: assume 49407 (OpenAI CLIP); fine for most OpenCLIP tokenizers too\n",
    "            self.eot_token = 49407\n",
    "\n",
    "    def forward(self, device):\n",
    "        # Build a batch of prompts with learned ctx + classname tokens\n",
    "        B = len(self.classnames)\n",
    "        ctx = self.ctx.to(device, dtype=self.dtype)                                # (ctx_len, dim)\n",
    "        ctx = ctx.unsqueeze(0).repeat(B, 1, 1)                                     # (B, ctx_len, dim)\n",
    "\n",
    "        tokens = self.classname_tokens.to(device)                                  # (B, seqlen)\n",
    "        with torch.no_grad():\n",
    "            class_embs = self.token_embedding(tokens).to(dtype=self.dtype)         # (B, seqlen, dim)\n",
    "\n",
    "        # Replace the first 'ctx_len' token slots *after* SOS with learned context.\n",
    "        # Layout: [SOS] [CTX x n] [rest of tokens ... EOT] [PAD ...]\n",
    "        # We fetch SOS position = 0\n",
    "        sos = class_embs[:, :1, :]\n",
    "        rest = class_embs[:, 1 + self.ctx_len:, :]                                 # drop slots weâ€™ll replace\n",
    "\n",
    "        x = torch.cat([sos, ctx, rest], dim=1)                                     # (B, T, dim)\n",
    "        x = x + self.positional_embedding[:x.size(1)].to(x)                        # add pos\n",
    "        x = x.permute(1, 0, 2)                                                     # NLD -> LND\n",
    "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)                                                     # LND -> NLD\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # take features at EOT position for each sequence\n",
    "        # find EOT index in our retokenized sequence (still valid)\n",
    "        eot_positions = (tokens == self.eot_token).argmax(dim=1)                   # (B,)\n",
    "        feat = x[torch.arange(B, device=device), eot_positions] @ self.text_projection\n",
    "\n",
    "        # normalize\n",
    "        feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        return feat\n",
    "\n",
    "# Instantiate CoOp head (learns only the context vectors)\n",
    "coop = SimpleCoOp(model, classnames, n_ctx=8).to(device)\n",
    "optim_coop = optim.AdamW([coop.ctx], lr=5e-3, weight_decay=0.0)\n",
    "epochs = 5\n",
    "\n",
    "# Precompute the model's logit scale (frozen)\n",
    "logit_scale = model.logit_scale.exp().to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_with_coop(model, coop_head, loader):\n",
    "    coop_head.eval()\n",
    "    correct = total = 0\n",
    "    # class text features from coop head:\n",
    "    class_text_features = coop_head(device)  # (num_classes, d)\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        img_f = model.encode_image(images)\n",
    "        img_f = img_f / img_f.norm(dim=-1, keepdim=True)\n",
    "        logits = (logit_scale * img_f @ class_text_features.T)\n",
    "        preds = logits.argmax(dim=-1).cpu()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(f\"CoOp few-shot eval: {100*correct/total:.2f}%\")\n",
    "\n",
    "# Train CoOp with cross-entropy using fixed image features\n",
    "ce = nn.CrossEntropyLoss()\n",
    "for ep in range(1, epochs+1):\n",
    "    coop.train()\n",
    "    total_loss = 0.0\n",
    "    for imgs, labels in fewshot_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            img_f = model.encode_image(imgs)\n",
    "            img_f = img_f / img_f.norm(dim=-1, keepdim=True)\n",
    "        txt_f = coop(device)                          # (C, d), recompute every step (cheap)\n",
    "        logits = (logit_scale * img_f @ txt_f.T)      # (B, C)\n",
    "        loss = ce(logits, labels)\n",
    "\n",
    "        optim_coop.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim_coop.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    print(f\"[CoOp] epoch {ep} | loss {total_loss/len(few_idx):.4f}\")\n",
    "    eval_with_coop(model, coop, loader)\n",
    "\n",
    "# After training, you can get improved text_features for future evals:\n",
    "with torch.no_grad():\n",
    "    text_features_coop = coop(device)  # use this instead of the original text_features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
