{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "-2NbWPlKLK4Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2NbWPlKLK4Q",
        "outputId": "fa21e5bd-6e11-486c-d2ad-aa7a1e35eb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch torchvision timm open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0cab695c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cab695c",
        "outputId": "76a1465f-8f4b-47c2-bae7-7c161a2c1dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zero-shot CIFAR-10 accuracy: 93.66%\n"
          ]
        }
      ],
      "source": [
        "import torch, open_clip\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# === Choose a model from recent CLIP-family work ===\n",
        "# Classic strong baseline:\n",
        "MODEL_NAME   = \"ViT-B-32\"\n",
        "PRETRAINED   = \"laion2b_s34b_b79k\"   # from OpenCLIP\n",
        "\n",
        "# Tip: try very recent ones too (if available in your env):\n",
        "# MODEL_NAME, PRETRAINED = \"ViT-SO400M-14-SigLIP\", \"webli\"        # SigLIP family\n",
        "# MODEL_NAME, PRETRAINED = \"EVA02-L-14\", \"laion2b_s9b_b144k\"      # EVA-CLIP family\n",
        "# (List available combos:)\n",
        "# import pprint; pprint.pp(open_clip.list_pretrained())\n",
        "\n",
        "# --- Load model + preprocess ---\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)\n",
        "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# --- Zero-shot on CIFAR-10 (tiny & quick) ---\n",
        "cifar = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=preprocess)\n",
        "loader = DataLoader(cifar, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "classnames = cifar.classes\n",
        "\n",
        "prompts = [f\"a photo of a {c}\" for c in classnames]\n",
        "with torch.no_grad():\n",
        "    text_tokens   = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        image_features = model.encode_image(images)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        # CLIP-style scaled cosine sims\n",
        "        logits = 100.0 * image_features @ text_features.T\n",
        "        preds = logits.argmax(dim=-1).cpu()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Zero-shot CIFAR-10 accuracy: {100*correct/total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "25a428dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25a428dd",
        "outputId": "8308a1ed-23df-45e9-958b-630adf029ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 - train CE loss: 0.1261\n",
            "Epoch 2/5 - train CE loss: 0.1059\n",
            "Epoch 3/5 - train CE loss: 0.1009\n",
            "Epoch 4/5 - train CE loss: 0.0979\n",
            "Epoch 5/5 - train CE loss: 0.0954\n",
            "CoOp CIFAR-10 accuracy (n_ctx=16, epochs=5): 96.62%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CoOpPrompt(nn.Module):\n",
        "    def __init__(self, clip_model, tokenizer, classnames, n_ctx=16, init_scale=0.02, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.model = clip_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.classnames = classnames\n",
        "        self.device = device\n",
        "\n",
        "        # Text transformer sizes\n",
        "        self.context_length = getattr(self.model, \"context_length\", 77)\n",
        "        self.width = self.model.token_embedding.weight.shape[1]\n",
        "\n",
        "        # Learnable context (n_ctx continuous \"tokens\")\n",
        "        self.ctx = nn.Parameter(init_scale * torch.randn(n_ctx, self.width))\n",
        "\n",
        "        # Tokenize bare classnames (we'll prepend soft context ourselves)\n",
        "        with torch.no_grad():\n",
        "            self.class_token_ids = tokenizer(classnames).to(device)  # [C, L]\n",
        "            # EOT trick: in CLIP tokenization the EOT id is the largest id in each row\n",
        "            self.eot_indices = self.class_token_ids.argmax(dim=-1)   # [C]\n",
        "\n",
        "        # Freeze CLIP\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward_text_features(self):\n",
        "        C = len(self.classnames)\n",
        "        token_ids = self.class_token_ids  # [C, L]\n",
        "\n",
        "        # Frozen lookup for class tokens\n",
        "        with torch.no_grad():\n",
        "            tok_emb = self.model.token_embedding(token_ids)  # [C, L, W]\n",
        "\n",
        "        sos = tok_emb[:, :1, :]         # [C,1,W]\n",
        "        class_part = tok_emb[:, 1:, :]  # [C,L-1,W]\n",
        "\n",
        "        # Learnable context repeated across classes\n",
        "        ctx = self.ctx.unsqueeze(0).expand(C, -1, -1)  # [C, n_ctx, W]\n",
        "\n",
        "        # [SOS] + [CTX...CTX] + [class tokens...]\n",
        "        x = torch.cat([sos, ctx, class_part], dim=1)  # [C, 1+n_ctx+(L-1), W]\n",
        "\n",
        "        # Pad/truncate to model context length\n",
        "        L_target = getattr(self.model, \"context_length\", 77)\n",
        "        if x.size(1) > L_target:\n",
        "            x = x[:, :L_target, :]\n",
        "        elif x.size(1) < L_target:\n",
        "            pad_len = L_target - x.size(1)\n",
        "            pad = torch.zeros(C, pad_len, x.size(2), device=self.device, dtype=x.dtype)\n",
        "            x = torch.cat([x, pad], dim=1)\n",
        "        L = x.size(1)\n",
        "\n",
        "        # Dtypes, pos emb, and (optional) attn mask, all sliced to L\n",
        "        text_dtype = self.model.token_embedding.weight.dtype\n",
        "        pos = self.model.positional_embedding[:L].to(text_dtype)  # [L,W]\n",
        "\n",
        "        attn = getattr(self.model, \"attn_mask\", None)\n",
        "        if attn is not None:\n",
        "            attn = attn[:L, :L].to(text_dtype)  # ensure (L,L)\n",
        "\n",
        "        # Respect transformer.batch_first\n",
        "        batch_first = getattr(self.model.transformer, \"batch_first\", False)\n",
        "        if batch_first:\n",
        "            # want [N, L, D]\n",
        "            x = x.to(text_dtype) + pos.unsqueeze(0)   # [C,L,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # stays [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "        else:\n",
        "            # want [L, N, D]\n",
        "            x = (x.to(text_dtype) + pos).permute(1, 0, 2)  # [L,C,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # [L,C,W]\n",
        "            x = x.permute(1, 0, 2)                         # [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "\n",
        "        # Pool at EOT (shift by n_ctx), clamp in case of truncation\n",
        "        eot = (self.eot_indices + self.ctx.shape[0]).clamp(max=L - 1)\n",
        "        text_emb = x[torch.arange(C, device=self.device), eot] @ self.model.text_projection  # [C,D]\n",
        "\n",
        "        # Normalize\n",
        "        text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "        return text_emb\n",
        "\n",
        "\n",
        "\n",
        "# --- Train CoOp on CIFAR-10 train set (learn only the context vectors) ---\n",
        "def train_coop(\n",
        "    model, tokenizer, classnames, device=\"cpu\",\n",
        "    n_ctx=16, epochs=5, batch_size=256, lr=5e-3, num_workers=2\n",
        "):\n",
        "    coop = CoOpPrompt(model, tokenizer, classnames, n_ctx=n_ctx, device=device).to(device)\n",
        "    opt = torch.optim.AdamW([coop.ctx], lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Use the same preprocess for simplicity\n",
        "    train_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    model.eval()  # keep CLIP frozen & deterministic\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                img_feat = model.encode_image(images)\n",
        "                img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            txt_feat = coop.forward_text_features()  # recompute as ctx changes\n",
        "\n",
        "            logits = 100.0 * img_feat @ txt_feat.T\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{epochs} - train CE loss: {running_loss / len(train_set):.4f}\")\n",
        "\n",
        "    return coop\n",
        "\n",
        "# --- Evaluate with the learned CoOp prompts on CIFAR-10 test set ---\n",
        "@torch.no_grad()\n",
        "def eval_coop(model, coop, loader, device=\"cpu\"):\n",
        "    text_features = coop.forward_text_features()  # [C,D]\n",
        "\n",
        "    correct = total = 0\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        img_feat = model.encode_image(images)\n",
        "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        logits = 100.0 * img_feat @ text_features.T\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "# === Run it ===\n",
        "n_ctx = 16          # number of soft tokens (common CoOp default)\n",
        "epochs = 5          # quick run; increase for better results\n",
        "batch_size = 256\n",
        "lr = 5e-3\n",
        "\n",
        "coop = train_coop(model, tokenizer, classnames, device=device,\n",
        "                  n_ctx=n_ctx, epochs=epochs, batch_size=batch_size, lr=lr)\n",
        "\n",
        "# Reuse your existing test loader from the first cell if it's in scope; otherwise recreate it quickly:\n",
        "try:\n",
        "    loader\n",
        "except NameError:\n",
        "    test_set = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=preprocess)\n",
        "    loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "coop_acc = eval_coop(model, coop, loader, device=device)\n",
        "print(f\"CoOp CIFAR-10 accuracy (n_ctx={n_ctx}, epochs={epochs}): {coop_acc:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
