{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-2NbWPlKLK4Q",
      "metadata": {
        "id": "-2NbWPlKLK4Q"
      },
      "outputs": [],
      "source": [
        "# 1) (Optional) Installs â€” uncomment on first run\n",
        "# !pip install -U matplotlib timm torchvision torch tqdm scikit-learn --quiet\n",
        "!pip install -U open-clip-torch torchinfo --quiet\n",
        "print('If you need packages, uncomment the pip lines above and run this cell.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Common imports + seeds\n",
        "import math, random, os, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torchinfo\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import open_clip\n",
        "\n",
        "def set_seed(seed:int=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    #torch.backends.cudnn.deterministic = True\n",
        "    #torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Device is {device}\")"
      ],
      "metadata": {
        "id": "4B8-K5xhgjCh"
      },
      "id": "4B8-K5xhgjCh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# Pick a lightweight variant for quick experiments\n",
        "MODEL_NAME = 'MobileCLIP2-S0'   # alternatives: 'MobileCLIP2-S2', 'MobileCLIP2-B', 'MobileCLIP2-S3', 'MobileCLIP2-S4', 'MobileCLIP2-L-14'\n",
        "PRETRAINED = 'dfndr2b'          # per MobileCLIP2 release in OpenCLIP\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"ViT-B-32\"  # new MobileCLIP2 in open_clip\n",
        "    pretrained: str = \"laion2b_s34b_b79k\"\n",
        "    image_size: int = 224\n",
        "    batch_size: int = 256\n",
        "    num_workers: int = 4\n",
        "    max_epochs: int = 1\n",
        "\n",
        "    # LR split: base vs prompt\n",
        "    lr_base: float = 1e-5\n",
        "    lr_prompt: float = 5e-3\n",
        "    weight_decay: float = 0.05\n",
        "\n",
        "    # Unfreeze (if you want a light finetune on top of prompt)\n",
        "    unfreeze_layers: Tuple[str, ...] = tuple()  # e.g. (\"visual.transformer.resblocks.11\",)\n",
        "\n",
        "    # CoOp prompt length\n",
        "    prompt_len: int = 16\n",
        "\n",
        "    # Overfit sanity tiny subset\n",
        "    overfit_n_classes: int = 2\n",
        "    overfit_k_per_class: int = 8\n",
        "    overfit_epochs: int = 50\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ],
      "metadata": {
        "id": "XPUNz2engqAX"
      },
      "id": "XPUNz2engqAX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load model + preprocess ---\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(cfg.model_name, pretrained=cfg.pretrained)\n",
        "tokenizer = open_clip.get_tokenizer(cfg.model_name)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "torchinfo.summary(model)"
      ],
      "metadata": {
        "id": "e11fi4S_hLO8"
      },
      "id": "e11fi4S_hLO8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=preprocess)\n",
        "full_train_size = len(full_train_ds)\n",
        "val_size = int(full_train_size * 0.1)\n",
        "\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_train_ds, [full_train_size - val_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=False, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "classnames = full_train_ds.classes\n",
        "num_classes = len(classnames)\n",
        "\n",
        "print(f\"Val dataset:   {len(val_ds)} images, {num_classes} classes\")\n",
        "\n",
        "def show_batch(dl: DataLoader, rows: int=2, cols: int=8):\n",
        "    batch = next(iter(dl))\n",
        "    images, labels = batch\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            idx = i*cols + j\n",
        "            if idx >= len(images):\n",
        "                break\n",
        "            img = images[idx].permute(1, 2, 0).cpu().numpy()\n",
        "            label = classnames[labels[idx]]\n",
        "            axs[i, j].imshow(img)\n",
        "            axs[i, j].set_title(label)\n",
        "            axs[i, j].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_batch(val_loader, rows=2, cols=8)"
      ],
      "metadata": {
        "id": "ePMhT4KUimHu"
      },
      "id": "ePMhT4KUimHu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cab695c",
      "metadata": {
        "id": "0cab695c"
      },
      "outputs": [],
      "source": [
        "prompts = [f\"a photo of a {c}\" for c in classnames]\n",
        "with torch.no_grad():\n",
        "    text_tokens   = tokenizer(prompts).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "correct = total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "        image_features = model.encode_image(images)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        # CLIP-style scaled cosine sims\n",
        "        logits = 100.0 * image_features @ text_features.T\n",
        "        preds = logits.argmax(dim=-1).cpu()\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Zero-shot CIFAR-10 accuracy: {100*correct/total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a428dd",
      "metadata": {
        "id": "25a428dd"
      },
      "outputs": [],
      "source": [
        "class CoOpPrompt(nn.Module):\n",
        "    def __init__(self, clip_model, tokenizer, classnames, n_ctx=16, init_scale=0.02, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.model = clip_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.classnames = classnames\n",
        "        self.device = device\n",
        "\n",
        "        # Text transformer sizes\n",
        "        self.context_length = getattr(self.model, \"context_length\", 77)\n",
        "        self.width = self.model.token_embedding.weight.shape[1]\n",
        "\n",
        "        # Learnable context (n_ctx continuous \"tokens\")\n",
        "        self.ctx = nn.Parameter(init_scale * torch.randn(n_ctx, self.width))\n",
        "\n",
        "        # Tokenize bare classnames (we'll prepend soft context ourselves)\n",
        "        with torch.no_grad():\n",
        "            self.class_token_ids = tokenizer(classnames).to(device)  # [C, L]\n",
        "            # EOT trick: in CLIP tokenization the EOT id is the largest id in each row\n",
        "            self.eot_indices = self.class_token_ids.argmax(dim=-1)   # [C]\n",
        "\n",
        "        # Freeze CLIP\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward_text_features(self):\n",
        "        C = len(self.classnames)\n",
        "        token_ids = self.class_token_ids  # [C, L]\n",
        "\n",
        "        # Frozen lookup for class tokens\n",
        "        with torch.no_grad():\n",
        "            tok_emb = self.model.token_embedding(token_ids)  # [C, L, W]\n",
        "\n",
        "        sos = tok_emb[:, :1, :]         # [C,1,W]\n",
        "        class_part = tok_emb[:, 1:, :]  # [C,L-1,W]\n",
        "\n",
        "        # Learnable context repeated across classes\n",
        "        ctx = self.ctx.unsqueeze(0).expand(C, -1, -1)  # [C, n_ctx, W]\n",
        "\n",
        "        # [SOS] + [CTX...CTX] + [class tokens...]\n",
        "        x = torch.cat([sos, ctx, class_part], dim=1)  # [C, 1+n_ctx+(L-1), W]\n",
        "\n",
        "        # Pad/truncate to model context length\n",
        "        L_target = getattr(self.model, \"context_length\", 77)\n",
        "        if x.size(1) > L_target:\n",
        "            x = x[:, :L_target, :]\n",
        "        elif x.size(1) < L_target:\n",
        "            pad_len = L_target - x.size(1)\n",
        "            pad = torch.zeros(C, pad_len, x.size(2), device=self.device, dtype=x.dtype)\n",
        "            x = torch.cat([x, pad], dim=1)\n",
        "        L = x.size(1)\n",
        "\n",
        "        # Dtypes, pos emb, and (optional) attn mask, all sliced to L\n",
        "        text_dtype = self.model.token_embedding.weight.dtype\n",
        "        pos = self.model.positional_embedding[:L].to(text_dtype)  # [L,W]\n",
        "\n",
        "        attn = getattr(self.model, \"attn_mask\", None)\n",
        "        if attn is not None:\n",
        "            attn = attn[:L, :L].to(text_dtype)  # ensure (L,L)\n",
        "\n",
        "        # Respect transformer.batch_first\n",
        "        batch_first = getattr(self.model.transformer, \"batch_first\", False)\n",
        "        if batch_first:\n",
        "            # want [N, L, D]\n",
        "            x = x.to(text_dtype) + pos.unsqueeze(0)   # [C,L,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # stays [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "        else:\n",
        "            # want [L, N, D]\n",
        "            x = (x.to(text_dtype) + pos).permute(1, 0, 2)  # [L,C,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # [L,C,W]\n",
        "            x = x.permute(1, 0, 2)                         # [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "\n",
        "        # Pool at EOT (shift by n_ctx), clamp in case of truncation\n",
        "        eot = (self.eot_indices + self.ctx.shape[0]).clamp(max=L - 1)\n",
        "        text_emb = x[torch.arange(C, device=self.device), eot] @ self.model.text_projection  # [C,D]\n",
        "\n",
        "        # Normalize\n",
        "        text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "        return text_emb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_coop(model, coop, loader, device, loss_fn, desc=\"eval\", pbar=True):\n",
        "    \"\"\"\n",
        "    Eval loop shaped like evaluate.py, adapted to CLIP+CoOp.\n",
        "\n",
        "    model: frozen CLIP (open_clip)\n",
        "    coop:  prompt learner (provides forward_text_features())\n",
        "    loader: DataLoader yielding (images, labels)\n",
        "    device: torch.device or str\n",
        "    loss_fn: criterion (CrossEntropyLoss)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    loss_sum, correct, count = 0.0, 0, 0\n",
        "    iterator = tqdm(loader, desc=desc, leave=False) if pbar else loader\n",
        "\n",
        "    # ctx is fixed during eval; text feats can be computed once\n",
        "    txt_feat = coop.forward_text_features()  # [C, D]\n",
        "\n",
        "    for images, labels in iterator:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        img_feat = model.encode_image(images)                # [B, D]\n",
        "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "        logits = 100.0 * (img_feat @ txt_feat.T)             # [B, C]\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        bs = labels.size(0)\n",
        "        loss_sum += loss.item() * bs\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        count += bs\n",
        "\n",
        "        if pbar:\n",
        "            acc = (correct / max(1, count)) * 100.0\n",
        "            iterator.set_postfix(loss=loss.item(), acc=f\"{acc:.2f}%\")\n",
        "\n",
        "    avg_loss = loss_sum / max(1, count)\n",
        "    avg_acc = correct / max(1, count)\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def train_coop(\n",
        "    model, tokenizer, classnames, train_loader, val_loader, device=\"cpu\",\n",
        "    n_ctx=16, epochs=5, batch_size=256, lr=5e-3, num_workers=2,\n",
        "    val_fraction=0.1, log_interval=25, ema_alpha=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Train CoOp prompts on dataset using a tqdm-style loop like training_loop.py.\n",
        "    Only coop.ctx is optimized; CLIP remains frozen in eval().\n",
        "    \"\"\"\n",
        "    # --- 1) Module & optimizer\n",
        "    coop = CoOpPrompt(model, tokenizer, classnames, n_ctx=n_ctx, device=device).to(device)\n",
        "    optimizer = torch.optim.AdamW([coop.ctx], lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # --- 3) Freeze CLIP; we only learn coop.ctx\n",
        "    model.eval()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ---- train epoch (same rhythm as training_loop.py)\n",
        "        train_sum, train_correct, train_count = 0.0, 0, 0\n",
        "        ema = None\n",
        "\n",
        "        iterator = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {epoch}/{epochs} | train\",\n",
        "            leave=False,\n",
        "            dynamic_ncols=True,\n",
        "            mininterval=0.5,\n",
        "        )\n",
        "\n",
        "        for step, (images, labels) in enumerate(iterator, start=1):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # frozen image features\n",
        "            with torch.no_grad():\n",
        "                img_feat = model.encode_image(images)                  # [B, D]\n",
        "                img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # prompt-conditioned text features (recomputed as ctx updates)\n",
        "            txt_feat = coop.forward_text_features()                     # [C, D]\n",
        "            logits = 100.0 * (img_feat @ txt_feat.T)                    # [B, C]\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # stats\n",
        "            bs = labels.size(0)\n",
        "            train_sum += loss.item() * bs\n",
        "            preds = logits.argmax(dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            train_count += bs\n",
        "\n",
        "            # EMA of loss for tqdm postfix\n",
        "            ema = loss.item() if ema is None else (1 - ema_alpha) * ema + ema_alpha * loss.item()\n",
        "\n",
        "            if (step % log_interval == 0) or (step == len(iterator)):\n",
        "                acc_pct = 100.0 * train_correct / max(1, train_count)\n",
        "                iterator.set_postfix_str(f\"loss(ema)={ema:.4f}  acc={acc_pct:.2f}%\")\n",
        "\n",
        "        avg_train_loss = train_sum / max(1, train_count)\n",
        "        avg_train_acc  = train_correct / max(1, train_count)\n",
        "\n",
        "        # ---- validation using the separate evaluate (evaluate.py style)\n",
        "        avg_val_loss, avg_val_acc = evaluate_coop(\n",
        "            model, coop, val_loader, device, loss_fn=loss_fn, desc=\"valid\", pbar=True\n",
        "        )\n",
        "\n",
        "        # ---- log & history\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['train_acc'].append(avg_train_acc)\n",
        "        history['val_acc'].append(avg_val_acc)\n",
        "\n",
        "        tqdm.write(\n",
        "            f\"Epoch {epoch:03d}: \"\n",
        "            f\"train_loss={avg_train_loss:.4f}  train_acc={avg_train_acc*100:.2f}%  \"\n",
        "            f\"val_loss={avg_val_loss:.4f}  val_acc={avg_val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return coop, history"
      ],
      "metadata": {
        "id": "0VssWZSunzOL"
      },
      "id": "0VssWZSunzOL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coop, history = train_coop(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    classnames=classnames,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    n_ctx=cfg.prompt_len,\n",
        "    epochs=cfg.max_epochs,\n",
        "    batch_size=cfg.batch_size,\n",
        "    lr=cfg.lr_prompt,\n",
        "    num_workers=cfg.num_workers,\n",
        ")"
      ],
      "metadata": {
        "id": "qW0FTw_Ovjg0"
      },
      "id": "qW0FTw_Ovjg0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}