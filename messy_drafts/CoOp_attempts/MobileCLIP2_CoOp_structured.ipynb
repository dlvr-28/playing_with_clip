{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1) Install deps (ok to skip if already installed)\n",
        "!pip install -U torch torchvision timm open_clip_torch tqdm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2) Imports & seeds\n",
        "import os, math, random, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import open_clip\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3) Config (edit as needed for quick experiments)\n",
        "@dataclass\n",
        "class Config:\n",
        "    dataset_name: str = \"CIFAR-10\"\n",
        "    model_name: str = \"ViT-B-32\"\n",
        "    pretrained: str = \"laion2b_s34b_b79k\"\n",
        "    batch_size: int = 256\n",
        "    num_workers: int = 2\n",
        "    max_epochs: int = 5\n",
        "    prompt_len: int = 16\n",
        "    init_scale: float = 0.02\n",
        "    lr_prompt: float = 5e-3\n",
        "    weight_decay: float = 0.0\n",
        "    log_interval: int = 25\n",
        "    temperature: float = 100.0\n",
        "    overfit_subset: int = 64\n",
        "    seed: int = 42\n",
        "\n",
        "cfg = Config()\n",
        "cfg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4) Load base CLIP model + preprocess + tokenizer\n",
        "set_seed(cfg.seed)\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(cfg.model_name, pretrained=cfg.pretrained)\n",
        "tokenizer = open_clip.get_tokenizer(cfg.model_name)\n",
        "model = model.to(device).eval()\n",
        "print(\"Loaded:\", cfg.model_name, \"pretrained:\", cfg.pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5) CoOp module: learnable context (prompt) + wrapper that produces logits\n",
        "class CoOpPrompt(nn.Module):\n",
        "    def __init__(self, clip_model, tokenizer, classnames: List[str], n_ctx=16, init_scale=0.02, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Learnable continuous prompt for CLIP text tower (CoOp-style).\n",
        "\n",
        "        clip_model: open_clip model (frozen)\n",
        "        tokenizer: tokenizer for the model\n",
        "        classnames: list of class strings\n",
        "        n_ctx: number of soft tokens to prepend\n",
        "        init_scale: std of normal init for soft tokens\n",
        "\n",
        "        Shapes:\n",
        "            - ctx: [n_ctx, W]\n",
        "            - class_token_ids: [C, L]\n",
        "            - token embeddings: [C, L, W]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = clip_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.classnames = classnames\n",
        "        self.device = device\n",
        "\n",
        "        self.context_length = getattr(self.model, \"context_length\", 77)\n",
        "        self.width = self.model.token_embedding.weight.shape[1]\n",
        "\n",
        "        self.ctx = nn.Parameter(init_scale * torch.randn(n_ctx, self.width))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.class_token_ids = tokenizer(classnames).to(device)  # [C, L]\n",
        "            self.eot_indices = self.class_token_ids.argmax(dim=-1)   # [C]\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward_text_features(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            text_emb: [C, D] normalized embeddings for each class using current ctx\n",
        "        \"\"\"\n",
        "        C = len(self.classnames)\n",
        "        token_ids = self.class_token_ids  # [C, L]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tok_emb = self.model.token_embedding(token_ids)  # [C, L, W]\n",
        "\n",
        "        sos = tok_emb[:, :1, :]         # [C,1,W]\n",
        "        class_part = tok_emb[:, 1:, :]  # [C,L-1,W]\n",
        "\n",
        "        ctx = self.ctx.unsqueeze(0).expand(C, -1, -1)  # [C, n_ctx, W]\n",
        "\n",
        "        x = torch.cat([sos, ctx, class_part], dim=1)  # [C, 1+n_ctx+(L-1), W]\n",
        "\n",
        "        L_target = getattr(self.model, \"context_length\", 77)\n",
        "        if x.size(1) > L_target:\n",
        "            x = x[:, :L_target, :]\n",
        "        elif x.size(1) < L_target:\n",
        "            pad_len = L_target - x.size(1)\n",
        "            pad = torch.zeros(C, pad_len, x.size(2), device=self.device, dtype=x.dtype)\n",
        "            x = torch.cat([x, pad], dim=1)\n",
        "        L = x.size(1)\n",
        "\n",
        "        text_dtype = self.model.token_embedding.weight.dtype\n",
        "        pos = self.model.positional_embedding[:L].to(text_dtype)  # [L,W]\n",
        "        attn = getattr(self.model, \"attn_mask\", None)\n",
        "        if attn is not None:\n",
        "            attn = attn[:L, :L].to(text_dtype)\n",
        "\n",
        "        batch_first = getattr(self.model.transformer, \"batch_first\", False)\n",
        "        if batch_first:\n",
        "            x = x.to(text_dtype) + pos.unsqueeze(0)   # [C,L,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "        else:\n",
        "            x = (x.to(text_dtype) + pos).permute(1, 0, 2)  # [L,C,W]\n",
        "            x = self.model.transformer(x, attn_mask=attn)  # [L,C,W]\n",
        "            x = x.permute(1, 0, 2)                         # [C,L,W]\n",
        "            x = self.model.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "\n",
        "        eot = (self.eot_indices + self.ctx.shape[0]).clamp(max=L - 1)\n",
        "        text_emb = x[torch.arange(C, device=self.device), eot] @ self.model.text_projection  # [C,D]\n",
        "        text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "        return text_emb\n",
        "\n",
        "\n",
        "class PromptedCLIP(nn.Module):\n",
        "    def __init__(self, clip_model, prompt_learner: CoOpPrompt, temperature: float = 100.0):\n",
        "        \"\"\"\n",
        "        Wraps a frozen CLIP image tower + learnable prompt to produce class logits.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.clip = clip_model\n",
        "        self.prompt_learner = prompt_learner\n",
        "        self.temperature = temperature\n",
        "        for p in self.clip.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        images: [B, 3, H, W] -> logits: [B, C]\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_feat = self.clip.encode_image(images)  # [B,D]\n",
        "            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_feat = self.prompt_learner.forward_text_features()  # [C,D]\n",
        "        logits = self.temperature * img_feat @ text_feat.T       # [B,C]\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6) Data: CIFAR-10 train/val/test + overfit subset helper\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "tfm_train = preprocess\n",
        "tfm_eval = preprocess\n",
        "\n",
        "train_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm_train)\n",
        "test_set  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm_eval)\n",
        "\n",
        "val_set = Subset(test_set, list(range(0, 5000)))\n",
        "eval_set = Subset(test_set, list(range(5000, 10000)))\n",
        "\n",
        "classnames = train_set.classes\n",
        "print(\"Classes:\", classnames)\n",
        "\n",
        "def make_loaders(batch_size: int):\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
        "    test_loader  = DataLoader(eval_set,  batch_size=batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def make_overfit_loader(n: int = 64):\n",
        "    idx = list(range(n))\n",
        "    tiny = Subset(train_set, idx)\n",
        "    return DataLoader(tiny, batch_size=min(64, n), shuffle=True, num_workers=0, pin_memory=True)\n",
        "\n",
        "train_loader, val_loader, test_loader = make_loaders(cfg.batch_size)\n",
        "overfit_loader = make_overfit_loader(cfg.overfit_subset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7) Instantiate CoOp + optimizer (prompt params only)\n",
        "prompt_learner = CoOpPrompt(model, tokenizer, classnames, n_ctx=cfg.prompt_len, init_scale=cfg.init_scale, device=device).to(device)\n",
        "model_coop = PromptedCLIP(model, prompt_learner, temperature=cfg.temperature).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW([prompt_learner.ctx], lr=cfg.lr_prompt, weight_decay=cfg.weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Display trainable params\n",
        "sum(p.numel() for p in prompt_learner.parameters() if p.requires_grad), prompt_learner.ctx.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8) Train loop (prompt-only) with grad norm debug\n",
        "def train_prompt_only(model_coop: PromptedCLIP,\n",
        "                      opt: torch.optim.Optimizer,\n",
        "                      train_loader: DataLoader,\n",
        "                      val_loader: DataLoader,\n",
        "                      device: str,\n",
        "                      epochs: int,\n",
        "                      log_interval: int = 25,\n",
        "                      ema_alpha: float = 0.1,\n",
        "                      debug_grad: bool = True):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    prompt = model_coop.prompt_learner\n",
        "    model_coop.to(device)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model_coop.train()\n",
        "        train_sum, train_correct, train_count = 0.0, 0, 0\n",
        "        ema = None\n",
        "\n",
        "        iterator = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} | train\", leave=False, dynamic_ncols=True, mininterval=0.5)\n",
        "        for step, (images, labels) in enumerate(iterator, start=1):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model_coop(images)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            if debug_grad and (step % log_interval == 0 or step == len(iterator)):\n",
        "                with torch.no_grad():\n",
        "                    grad_norm = 0.0 if prompt.ctx.grad is None else float(prompt.ctx.grad.norm().item())\n",
        "                iterator.set_postfix_str(f\"loss={loss.item():.4f}  grad|ctx|={grad_norm:.2e}\")\n",
        "\n",
        "            opt.step()\n",
        "\n",
        "            bs = images.size(0)\n",
        "            train_sum += loss.item() * bs\n",
        "            preds = logits.argmax(dim=1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "            train_count += bs\n",
        "\n",
        "            ema = loss.item() if ema is None else (1 - ema_alpha) * ema + ema_alpha * loss.item()\n",
        "\n",
        "        avg_train_loss = train_sum / max(1, train_count)\n",
        "        avg_train_acc = train_correct / max(1, train_count)\n",
        "\n",
        "        v_loss, v_acc = evaluate_prompt_only(model_coop, val_loader, device=device, loss_fn=loss_fn, pbar=True)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(v_loss)\n",
        "        history['train_acc'].append(avg_train_acc)\n",
        "        history['val_acc'].append(v_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch:03d}: train_loss={avg_train_loss:.4f} train_acc={avg_train_acc*100:.2f}%  val_loss={v_loss:.4f} val_acc={v_acc*100:.2f}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 9) Evaluate + Plot\n",
        "@torch.no_grad()\n",
        "def evaluate_prompt_only(model_coop: PromptedCLIP, loader: DataLoader, device: str, loss_fn, desc=\"valid\", pbar=True):\n",
        "    model_coop.eval()\n",
        "    loss_sum, correct, count = 0.0, 0, 0\n",
        "    iterator = tqdm(loader, desc=desc, leave=False) if pbar else loader\n",
        "\n",
        "    for images, labels in iterator:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model_coop(images)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        bs = images.size(0)\n",
        "        loss_sum += loss.item() * bs\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        count += bs\n",
        "\n",
        "        if pbar:\n",
        "            acc = (correct / max(1, count)) * 100.0\n",
        "            iterator.set_postfix(loss=loss.item(), acc=f\"{acc:.2f}%\")\n",
        "\n",
        "    return loss_sum / max(1, count), correct / max(1, count)\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
        "\n",
        "    ax = axes[0]\n",
        "    ax.plot(epochs, history['train_loss'], label='train')\n",
        "    ax.plot(epochs, history['val_loss'], label='val')\n",
        "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.set_title('Loss'); ax.legend()\n",
        "\n",
        "    ax = axes[1]\n",
        "    if 'train_acc' in history and 'val_acc' in history:\n",
        "        ax.plot(epochs, [x * 100 for x in history['train_acc']], label='train')\n",
        "        ax.plot(epochs, [x * 100 for x in history['val_acc']], label='val')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No accuracy in history', ha='center', va='center', transform=ax.transAxes)\n",
        "    ax.set_xlabel('Epoch'); ax.set_title('Accuracy'); ax.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 10) Zero-shot baseline (optional quick check)\n",
        "@torch.no_grad()\n",
        "def zero_shot_acc(clip_model, tokenizer, classnames, loader, device=\"cpu\", temperature=100.0):\n",
        "    prompts = [f\"a photo of a {c}\" for c in classnames]\n",
        "    text_tokens = tokenizer(prompts).to(device)\n",
        "    text_features = clip_model.encode_text(text_tokens)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    correct = total = 0\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        image_features = clip_model.encode_image(images)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        logits = temperature * image_features @ text_features.T\n",
        "        preds = logits.argmax(dim=-1).cpu()\n",
        "        correct += (preds == labels.cpu()).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "test_loader_full = DataLoader(eval_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "print(\"Zero-shot acc (eval split):\", f\"{zero_shot_acc(model, tokenizer, classnames, test_loader_full, device=device, temperature=cfg.temperature):.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 11) Sanity check: overfit tiny subset\n",
        "prompt_learner_oc = CoOpPrompt(model, tokenizer, classnames, n_ctx=cfg.prompt_len, init_scale=cfg.init_scale, device=device).to(device)\n",
        "model_overfit = PromptedCLIP(model, prompt_learner_oc, temperature=cfg.temperature).to(device)\n",
        "opt_overfit = torch.optim.AdamW([prompt_learner_oc.ctx], lr=cfg.lr_prompt, weight_decay=cfg.weight_decay)\n",
        "\n",
        "tiny_loader = overfit_loader\n",
        "val_tiny_loader = DataLoader(Subset(test_set, list(range(cfg.overfit_subset))), batch_size=cfg.overfit_subset, shuffle=False)\n",
        "\n",
        "hist_tiny = train_prompt_only(model_overfit, opt_overfit, tiny_loader, val_tiny_loader, device=device, epochs=5, log_interval=5, debug_grad=True)\n",
        "plot_history(hist_tiny)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 12) Train on full train set, validate on val, evaluate on eval split\n",
        "prompt_learner = CoOpPrompt(model, tokenizer, classnames, n_ctx=cfg.prompt_len, init_scale=cfg.init_scale, device=device).to(device)\n",
        "model_coop = PromptedCLIP(model, prompt_learner, temperature=cfg.temperature).to(device)\n",
        "opt = torch.optim.AdamW([prompt_learner.ctx], lr=cfg.lr_prompt, weight_decay=cfg.weight_decay)\n",
        "\n",
        "history = train_prompt_only(model_coop, opt, train_loader, val_loader, device=device, epochs=cfg.max_epochs, log_interval=cfg.log_interval, debug_grad=True)\n",
        "plot_history(history)\n",
        "\n",
        "test_loss, test_acc = evaluate_prompt_only(model_coop, test_loader, device=device, loss_fn=loss_fn, desc=\"test\", pbar=True)\n",
        "print(f\"Test: loss={test_loss:.4f}, acc={test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 13) Extra analysis (example: ctx norms)\n",
        "with torch.no_grad():\n",
        "    ctx = model_coop.prompt_learner.ctx.detach().cpu()\n",
        "print(\"Ctx param shape:\", tuple(ctx.shape), \"  mean|ctx|:\", float(ctx.norm(dim=1).mean()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}