{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# MobileCLIP2 + CoOp (Context Optimization)\n", "\n", "This notebook starts from **MobileCLIP2** via `open_clip_torch` and adds a simple **Context Optimization (CoOp)** prompt learner, following the project's **STYLEGUIDE**.\n", "\n", "**Notebook flow** (from STYLEGUIDE):\n", "1. `!pip install` (only if needed)\n", "2. Common imports + seeds\n", "3. Load the model / pretrained model (+ preprocessors / related utilities)\n", "4. Create own model module with clear separation of base vs add-ons\n", "5. Hyperparameters section\n", "6. Optimizer + Schedule\n", "7. Train\n", "8. Plot History\n", "9. Evaluate + Show final result\n", "10. Extra analysis and visualisation\n", "\n", "> **References**: MobileCLIP2 in OpenCLIP (`MobileCLIP2-S{0,2,3,4}, B, L-14` with `pretrained='dfndr2b'`) and CoOp (Zhou et al.).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": false}, "outputs": [], "source": ["# 1) (Optional) Installs \u2014 uncomment on first run\n", "# !pip install -U open-clip-torch timm torchvision torch --quiet\n", "# !pip install -U scikit-learn tqdm matplotlib --quiet\n", "print('If you need packages, uncomment the pip lines above and run this cell.')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 2) Common imports + seeds\n", "import math, random, os, time\n", "from dataclasses import dataclass\n", "from typing import Tuple, List\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import DataLoader\n", "\n", "import torchvision\n", "from torchvision import datasets\n", "\n", "import matplotlib.pyplot as plt\n", "from tqdm import tqdm\n", "\n", "import open_clip\n", "\n", "def set_seed(seed:int=42):\n", "    random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "    torch.backends.cudnn.deterministic = True\n", "    torch.backends.cudnn.benchmark = False\n", "\n", "set_seed(123)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 3) Load MobileCLIP2 from OpenCLIP + transforms\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "\n", "# Pick a lightweight variant for quick experiments\n", "MODEL_NAME = 'MobileCLIP2-S0'   # alternatives: 'MobileCLIP2-S2', 'MobileCLIP2-B', 'MobileCLIP2-S3', 'MobileCLIP2-S4', 'MobileCLIP2-L-14'\n", "PRETRAINED = 'dfndr2b'          # per MobileCLIP2 release in OpenCLIP\n", "\n", "# Returns: (model, image_preprocess, text_preprocess)\n", "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)\n", "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n", "\n", "# Important for MobileCLIP(2): keep eval for frozen BN layers; we will train only prompt params\n", "model.eval()\n", "model.to(device)\n", "\n", "print('Loaded', MODEL_NAME, 'on', device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 4) Data: small ImageFolder or CIFAR-10 for a quick sanity check\n", "#    Using preprocess from OpenCLIP to ensure normalization matches the backbone.\n", "\n", "USE_CIFAR10 = True  # set False and edit DATA_ROOT to use your own ImageFolder\n", "DATA_ROOT = './data'  # Only used if USE_CIFAR10=False; expects train/ and val/ subfolders\n", "BATCH_SIZE = 64\n", "NUM_WORKERS = 4\n", "\n", "if USE_CIFAR10:\n", "    train_raw = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=True, download=True)\n", "    test_raw  = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=False, download=True)\n", "\n", "    # Wrap datasets to apply OpenCLIP preprocess lazily\n", "    class Preprocessed(torch.utils.data.Dataset):\n", "        def __init__(self, base):\n", "            self.base = base\n", "            self.classes = base.classes if hasattr(base, 'classes') else base.targets\n", "        def __len__(self):\n", "            return len(self.base)\n", "        def __getitem__(self, idx):\n", "            img, y = self.base[idx]\n", "            # preprocess: PIL -> tensor normalized to CLIP space\n", "            return preprocess(img), y\n", "\n", "    train_ds = Preprocessed(train_raw)\n", "    val_ds   = Preprocessed(test_raw)  # use test as validation for demo\n", "    test_ds  = Preprocessed(test_raw)\n", "    classnames = train_raw.classes\n", "else:\n", "    # ImageFolder usage; structure:\n", "    # DATA_ROOT/\n", "    #   train/<class_name>/*.jpg\n", "    #   val/<class_name>/*.jpg\n", "    train_ds = datasets.ImageFolder(os.path.join(DATA_ROOT, 'train'), transform=preprocess)\n", "    val_ds   = datasets.ImageFolder(os.path.join(DATA_ROOT, 'val'), transform=preprocess)\n", "    test_ds  = val_ds\n", "    classnames = train_ds.classes\n", "\n", "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n", "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n", "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n", "\n", "print(f\"Classes ({len(classnames)}):\", classnames[:10], '...')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Model module: base (frozen MobileCLIP2) + CoOp prompt learner\n", "\n", "- We keep the CLIP backbone **frozen** and in **eval** mode (BatchNorm stability on MobileCLIP2). \n", "- We learn **M** context vectors `ctx` (\"soft words\") that prepend the class name tokens.\n", "- Shapes are annotated per STYLEGUIDE."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PromptLearner(nn.Module):\n", "    def __init__(self, clip_model, tokenizer, classnames: List[str], n_ctx: int = 4, ctx_init: str = \"a photo of\"):\n", "        \"\"\"\n", "        Context Optimization (CoOp) prompt learner (unified context).\n", "\n", "        clip_model: OpenCLIP model with attributes used in encode_text\n", "        tokenizer: open_clip tokenizer for the chosen model\n", "        classnames: list[str] of dataset class names\n", "        n_ctx: number of learnable context tokens (M)\n", "        ctx_init: optional string to initialize context from (e.g., 'a photo of')\n", "        \"\"\"\n", "        super().__init__()\n", "        self.tokenizer = tokenizer\n", "        self.clip = clip_model\n", "        self.classnames = classnames\n", "        self.n_ctx = n_ctx\n", "\n", "        # Build tokenized prompts with room for M learnable vectors after the SOS token\n", "        # Template: [SOS] + M * [CTX] + tokens(classname) + [EOS] + padding ... up to context_length\n", "        # We'll override the embeddings at the M positions, not the token ids themselves.\n", "        template = \"{}\"  # CoOp uses learnable context instead of a fixed template like 'a photo of a {}'\n", "\n", "        prompts = [template.format(name.replace('_', ' ')) for name in classnames]\n", "        tokenized = self.tokenizer(prompts)  # [C, L]\n", "        self.register_buffer('prompts_tokens', tokenized, persistent=False)\n", "\n", "        # Init context vectors in the same embedding space as CLIP's token embeddings\n", "        ctx_dim = self.clip.text_projection.shape[1] if hasattr(self.clip, 'text_projection') else self.clip.text.text_projection.shape[1]\n", "        # Better: use token embedding dim\n", "        ctx_dim = self.clip.token_embedding.embedding_dim\n", "\n", "        if ctx_init is not None and len(ctx_init) > 0:\n", "            # Initialize from actual text, then take the first M token embeddings (after SOS)\n", "            init_ids = self.tokenizer([ctx_init])  # [1, L]\n", "            with torch.no_grad():\n", "                init_emb = self.clip.token_embedding(init_ids.to(self.clip.token_embedding.weight.device))  # [1, L, D]\n", "            # exclude first token ([SOS]) and take next M positions, pad if shorter\n", "            ctx_init_vecs = torch.zeros(self.n_ctx, ctx_dim, device=init_emb.device)\n", "            take = min(self.n_ctx, init_emb.shape[1]-1)\n", "            if take > 0:\n", "                ctx_init_vecs[:take] = init_emb[0, 1:1+take]\n", "            self.ctx = nn.Parameter(ctx_init_vecs)  # [M, D]\n", "        else:\n", "            self.ctx = nn.Parameter(torch.randn(self.n_ctx, ctx_dim) * 0.02)  # [M, D]\n", "\n", "    @torch.no_grad()\n", "    def set_classnames(self, classnames: List[str]):\n", "        self.classnames = classnames\n", "        template = \"{}\"\n", "        prompts = [template.format(name.replace('_',' ')) for name in classnames]\n", "        tokenized = self.tokenizer(prompts)\n", "        self.prompts_tokens = tokenized.to(self.prompts_tokens.device)\n", "\n", "    def forward(self):\n", "        \"\"\"\n", "        Returns the normalized text features for all classes using current context.\n", "        Output shape: [C, D] where C=#classes, D=text feature dim.\n", "        \"\"\"\n", "        tokens = self.prompts_tokens.to(self.clip.positional_embedding.device)  # [C, L]\n", "        cast_dtype = self.clip.transformer.get_cast_dtype()\n", "\n", "        # 1) Token Embeddings\n", "        x = self.clip.token_embedding(tokens).to(cast_dtype)  # [C, L, D]\n", "\n", "        # 2) Replace positions 1..M with learnable context (broadcast over classes)\n", "        #    tokens dims: [C, L] ; x dims: [C, L, D] ; ctx: [M, D]\n", "        x[:, 1:1+self.n_ctx, :] = self.ctx.unsqueeze(0).to(cast_dtype)  # [C, M, D]\n", "\n", "        # 3) Add positional embedding and run transformer\n", "        x = x + self.clip.positional_embedding.to(cast_dtype)  # [C, L, D]\n", "        x = x.permute(1, 0, 2)  # [L, C, D]\n", "        x = self.clip.transformer(x, attn_mask=self.clip.attn_mask)  # [L, C, D]\n", "        x = x.permute(1, 0, 2)  # [C, L, D]\n", "        x = self.clip.ln_final(x)  # [C, L, D]\n", "\n", "        # 4) Take features at the EOT position (OpenCLIP uses argmax trick)\n", "        eot = tokens.argmax(dim=-1)  # [C]\n", "        x = x[torch.arange(x.shape[0]), eot]  # [C, D]\n", "        x = x @ self.clip.text_projection  # [C, D_out]\n", "        x = x / x.norm(dim=-1, keepdim=True)\n", "        return x  # [C, D_out]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PromptedMobileCLIP(nn.Module):\n", "    def __init__(self, base_clip, tokenizer, classnames: List[str], n_ctx:int=4):\n", "        super().__init__()\n", "        self.base = base_clip\n", "        for p in self.base.parameters():\n", "            p.requires_grad = False  # freeze\n", "        self.base.eval()  # BN stability\n", "\n", "        self.prompt_learner = PromptLearner(self.base, tokenizer, classnames, n_ctx=n_ctx)\n", "\n", "    def forward(self, images):\n", "        \"\"\"\n", "        images: [B, 3, H, W] preprocessed for CLIP\n", "        returns logits over classes: [B, C]\n", "        \"\"\"\n", "        with torch.no_grad():\n", "            img_feat = self.base.encode_image(images)  # [B, D]\n", "            img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n", "\n", "        txt_feat = self.prompt_learner()              # [C, D]\n", "        logit_scale = self.base.logit_scale.exp()\n", "        logits = logit_scale * img_feat @ txt_feat.t()  # [B, C]\n", "        return logits\n", "\n", "    @torch.no_grad()\n", "    def zeroshot_classifier(self, classnames: List[str], template: str = \"a photo of a {}\"):\n", "        # Helper to compare CoOp vs classic hand-crafted prompt\n", "        prompts = [template.format(c.replace('_',' ')) for c in classnames]\n", "        tokens = tokenizer(prompts).to(self.base.positional_embedding.device)\n", "        feat = self.base.encode_text(tokens)\n", "        return feat / feat.norm(dim=-1, keepdim=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 5) Hyperparameters (Config)\n", "from dataclasses import dataclass\n", "\n", "@dataclass\n", "class Config:\n", "    dataset_name: str = 'CIFAR-10'\n", "    batch_size: int = BATCH_SIZE\n", "    num_workers: int = NUM_WORKERS\n", "    max_epochs: int = 3\n", "    lr_prompt: float = 5e-3   # typically higher LR for prompt params\n", "    weight_decay: float = 0.0 # prompt params usually without WD\n", "    prompt_len: int = 4       # CoOp: number of learnable context tokens\n", "\n", "cfg = Config()\n", "cfg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 6) Optimizer (prompt params only)\n", "model_coop = PromptedMobileCLIP(model, tokenizer, classnames, n_ctx=cfg.prompt_len).to(device)\n", "opt = torch.optim.AdamW(model_coop.prompt_learner.parameters(), lr=cfg.lr_prompt, weight_decay=cfg.weight_decay)\n", "loss_fn = nn.CrossEntropyLoss()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 7) Train loop (EMA of loss in progress bar)\n", "def evaluate(model, loader, device, loss_fn, desc=\"eval\", pbar=True):\n", "    model.eval()\n", "    loss_sum, correct, count = 0.0, 0, 0\n", "    it = tqdm(loader, desc=desc, leave=False) if pbar else loader\n", "    with torch.no_grad():\n", "        for data, target in it:\n", "            data = data.to(device, non_blocking=True)\n", "            target = target.to(device, non_blocking=True)\n", "            output = model(data)\n", "            loss = loss_fn(output, target)\n", "            bs = data.size(0)\n", "            loss_sum += loss.item() * bs\n", "            preds = output.argmax(dim=1)\n", "            correct += (preds == target).sum().item()\n", "            count += bs\n", "            if pbar:\n", "                acc = (correct / max(1, count)) * 100.0\n", "                it.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc:.2f}%\")\n", "    return loss_sum / max(1, count), correct / max(1, count)\n", "\n", "def train(model, train_loader, val_loader, optimizer, device, epochs:int, log_interval:int=25, ema_alpha:float=0.1):\n", "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n", "    for epoch in range(1, epochs + 1):\n", "        model.train()\n", "        train_sum, train_correct, train_count = 0.0, 0, 0\n", "        ema = None\n", "        iterator = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} | train\", leave=False, dynamic_ncols=True, mininterval=0.5)\n", "        for step, (data, target) in enumerate(iterator, start=1):\n", "            data = data.to(device, non_blocking=True)\n", "            target = target.to(device, non_blocking=True)\n", "            optimizer.zero_grad(set_to_none=True)\n", "            output = model(data)                     # [B, C]\n", "            loss = loss_fn(output, target)\n", "            loss.backward()\n", "            optimizer.step()\n", "            # Stats\n", "            bs = data.size(0)\n", "            train_sum += loss.item() * bs\n", "            preds = output.argmax(dim=1)\n", "            train_correct += (preds == target).sum().item()\n", "            train_count += bs\n", "            ema = loss.item() if ema is None else (1-ema_alpha)*ema + ema_alpha*loss.item()\n", "            if (step % log_interval == 0) or (step == len(iterator)):\n", "                acc_pct = 100.0 * train_correct / max(1, train_count)\n", "                iterator.set_postfix_str(f\"loss(ema)={ema:.4f} acc={acc_pct:.2f}%\")\n", "        avg_train_loss = train_sum / max(1, train_count)\n", "        avg_train_acc = train_correct / max(1, train_count)\n", "        avg_val_loss, avg_val_acc = evaluate(model, val_loader, device, loss_fn=loss_fn, desc=\"valid\", pbar=True)\n", "        history['train_loss'].append(avg_train_loss)\n", "        history['val_loss'].append(avg_val_loss)\n", "        history['train_acc'].append(avg_train_acc)\n", "        history['val_acc'].append(avg_val_acc)\n", "        tqdm.write(\n", "            f\"Epoch {epoch:03d}: train_loss={avg_train_loss:.4f} train_acc={avg_train_acc*100:.2f}% \"\n", "            f\"val_loss={avg_val_loss:.4f} val_acc={avg_val_acc*100:.2f}%\"\n", "        )\n", "    return history\n", "\n", "history = train(model_coop, train_loader, val_loader, opt, device, epochs=cfg.max_epochs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 8) Plot history (loss + accuracy)\n", "def plot_history(history):\n", "    epochs = range(1, len(history['train_loss']) + 1)\n", "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n", "    ax = axes[0]\n", "    ax.plot(epochs, history['train_loss'], label='train')\n", "    ax.plot(epochs, history['val_loss'], label='val')\n", "    ax.set_xlabel('Epoch') ; ax.set_ylabel('Loss') ; ax.set_title('Loss') ; ax.legend()\n", "    ax = axes[1]\n", "    if 'train_acc' in history and 'val_acc' in history:\n", "        ax.plot(epochs, [x * 100 for x in history['train_acc']], label='train')\n", "        ax.plot(epochs, [x * 100 for x in history['val_acc']], label='val')\n", "        ax.set_ylabel('Accuracy (%)')\n", "    else:\n", "        ax.text(0.5, 0.5, 'No accuracy in history', ha='center', va='center', transform=ax.transAxes)\n", "    ax.set_xlabel('Epoch') ; ax.set_title('Accuracy') ; ax.legend()\n", "    plt.show()\n", "\n", "plot_history(history)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 9) Evaluate on test set & show a few predictions\n", "test_loss, test_acc = evaluate(model_coop, test_loader, device, loss_fn, desc='test')\n", "print(f\"Test: loss={test_loss:.4f} acc={test_acc*100:.2f}%\")\n", "\n", "from itertools import islice\n", "model_coop.eval()\n", "samples = list(islice(iter(test_loader), 1))[0]\n", "imgs, labels = samples[0].to(device), samples[1]\n", "with torch.no_grad():\n", "    logits = model_coop(imgs)\n", "preds = logits.argmax(dim=1).cpu()\n", "\n", "plt.figure(figsize=(10,3))\n", "for i in range(8):\n", "    plt.subplot(2,4,i+1)\n", "    # un-normalize for visualization (approx):\n", "    img = imgs[i].cpu().permute(1,2,0).float()\n", "    img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n", "    plt.imshow(img)\n", "    plt.title(f\"gt:{classnames[labels[i]]}\\npred:{classnames[preds[i]]}\")\n", "    plt.axis('off')\n", "plt.tight_layout(); plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 10) Extra analysis: (optional) confusion matrix if sklearn is available\n", "try:\n", "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n", "    y_true, y_pred = [], []\n", "    with torch.no_grad():\n", "        for x, y in tqdm(test_loader, leave=False, desc='cm'):\n", "            p = model_coop(x.to(device)).argmax(dim=1).cpu()\n", "            y_true.extend(y.cpu().tolist())\n", "            y_pred.extend(p.tolist())\n", "    cm = confusion_matrix(y_true, y_pred)\n", "    disp = ConfusionMatrixDisplay(cm, display_labels=classnames)\n", "    fig, ax = plt.subplots(figsize=(8,8))\n", "    disp.plot(ax=ax, include_values=False, xticks_rotation=90, cmap='Blues')\n", "    plt.title('Confusion Matrix (test)')\n", "    plt.tight_layout(); plt.show()\n", "except Exception as e:\n", "    print('Install scikit-learn to see confusion matrix. Skipping. Error:', e)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notes\n", "- The backbone is **frozen**; only the prompt parameters `ctx` (shape `[M, D]`) are trained.\n", "- For MobileCLIP(2), keeping the model in **eval()** avoids BN updates.\n", "- You can change `cfg.prompt_len` to explore different context lengths (e.g., 4, 8, 16).\n", "- To compare with classic prompts, see `zeroshot_classifier()` helper."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}