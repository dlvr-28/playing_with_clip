{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRp91S7wJsBE"
      },
      "source": [
        "# MobileCLIP2 + CoOp (Context Optimization)\n",
        "\n",
        "This notebook starts from **MobileCLIP2** via `open_clip_torch` and adds a simple **Context Optimization (CoOp)** prompt learner, following the project's **STYLEGUIDE**.\n",
        "\n",
        "**Notebook flow** (from STYLEGUIDE):\n",
        "1. `!pip install` (only if needed)\n",
        "2. Common imports + seeds\n",
        "3. Load the model / pretrained model (+ preprocessors / related utilities)\n",
        "4. Create own model module with clear separation of base vs add-ons\n",
        "5. Hyperparameters section\n",
        "6. Optimizer + Schedule\n",
        "7. Train\n",
        "8. Plot History\n",
        "9. Evaluate + Show final result\n",
        "10. Extra analysis and visualisation\n",
        "\n",
        "> **References**: MobileCLIP2 in OpenCLIP (`MobileCLIP2-S{0,2,3,4}, B, L-14` with `pretrained='dfndr2b'`) and CoOp (Zhou et al.).\n"
      ],
      "id": "SRp91S7wJsBE"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP7cpsQ8JsBH",
        "outputId": "a174b596-2d8c-4a5c-84dc-f7bbe7779aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: open-clip-torch in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.35.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open-clip-torch) (1.0.19)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open-clip-torch) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open-clip-torch) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open-clip-torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open-clip-torch) (1.1.10)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open-clip-torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open-clip-torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open-clip-torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open-clip-torch) (2025.8.3)\n",
            "If you need packages, uncomment the pip lines above and run this cell.\n"
          ]
        }
      ],
      "source": [
        "# 1) (Optional) Installs — uncomment on first run\n",
        "# !pip install -U matplotlib tqdm timm torchvision torch --quiet\n",
        "!pip install -U scikit-learn open-clip-torch\n",
        "print('If you need packages, uncomment the pip lines above and run this cell.')"
      ],
      "id": "yP7cpsQ8JsBH"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1SOv1nnJsBI",
        "outputId": "de470325-cc58-4bdd-ef51-15e3be40427d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runnin on cuda\n"
          ]
        }
      ],
      "source": [
        "# 2) Common imports + seeds\n",
        "import math, random, os, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import open_clip\n",
        "\n",
        "def set_seed(seed:int=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Runnin on {device}\")\n",
        "\n",
        "try:\n",
        "    import multiprocessing as mp\n",
        "    if mp.get_start_method(allow_none=True) != \"spawn\":\n",
        "        mp.set_start_method(\"spawn\", force=True)\n",
        "except RuntimeError:\n",
        "    pass  # already set\n"
      ],
      "id": "k1SOv1nnJsBI"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0317abaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0317abaa",
        "outputId": "2950f348-d820-4aa2-8f86-ab1635670755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(model_name='MobileCLIP2-S0', pretrained='dfndr2b', image_size=224, batch_size=64, num_workers=4, max_epochs=5, lr_base=1e-05, lr_prompt=0.001, weight_decay=0.05, unfreeze_layers=(), prompt_len=4, overfit_n_classes=2, overfit_k_per_class=8, overfit_epochs=50)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# Pick a lightweight variant for quick experiments\n",
        "# MODEL_NAME = 'MobileCLIP2-S0'   # alternatives: 'MobileCLIP2-S2', 'MobileCLIP2-B', 'MobileCLIP2-S3', 'MobileCLIP2-S4', 'MobileCLIP2-L-14'\n",
        "# PRETRAINED = 'dfndr2b'          # per MobileCLIP2 release in OpenCLIP\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"MobileCLIP2-S0\"\n",
        "    pretrained: str = \"dfndr2b\"\n",
        "    image_size: int = 224\n",
        "    batch_size: int = 64\n",
        "    num_workers: int = 4\n",
        "    max_epochs: int = 5\n",
        "\n",
        "    # LR split: base vs prompt\n",
        "    lr_base: float = 1e-5\n",
        "    lr_prompt: float = 1e-3\n",
        "    weight_decay: float = 0.05\n",
        "\n",
        "    # Unfreeze (if you want a light finetune on top of prompt)\n",
        "    unfreeze_layers: Tuple[str, ...] = tuple()  # e.g. (\"visual.transformer.resblocks.11\",)\n",
        "\n",
        "    # CoOp prompt length\n",
        "    prompt_len: int = 4\n",
        "\n",
        "    # Overfit sanity tiny subset\n",
        "    overfit_n_classes: int = 2\n",
        "    overfit_k_per_class: int = 8\n",
        "    overfit_epochs: int = 50\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YLvq5VpZJsBJ"
      },
      "outputs": [],
      "source": [
        "# open_clip provides: model, preprocess\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "    cfg.model_name, pretrained=cfg.pretrained, device=device\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer(cfg.model_name)"
      ],
      "id": "YLvq5VpZJsBJ"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edzP4R0DJsBK",
        "outputId": "8e0c581e-486e-450f-babb-3cdd0b36d766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tfm_train = transforms.Compose([\n",
        "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711)),\n",
        "])\n",
        "tfm_eval = transforms.Compose([\n",
        "    transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711)),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"data\", train=True,  transform=tfm_train, download=True)\n",
        "val_ds   = datasets.CIFAR10(root=\"data\", train=False, transform=tfm_eval,  download=True)\n",
        "classnames = train_ds.classes  # list[str]\n",
        "num_classes = len(classnames)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False,\n",
        "                          num_workers=cfg.num_workers, pin_memory=True)\n"
      ],
      "id": "edzP4R0DJsBK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58VUOFD0JsBK"
      },
      "source": [
        "## 4) Model module: base (frozen MobileCLIP2) + CoOp prompt learner\n",
        "\n",
        "- We keep the CLIP backbone **frozen** and in **eval** mode (BatchNorm stability on MobileCLIP2).\n",
        "- We learn **M** context vectors `ctx` (\"soft words\") that prepend the class name tokens.\n",
        "- Shapes are annotated per STYLEGUIDE."
      ],
      "id": "58VUOFD0JsBK"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F2VS5APMJsBK"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, text_encoder, tokenizer, classnames, n_ctx=16, prefix=\"a photo of\", suffix=\"\"):\n",
        "        super().__init__()\n",
        "        self.classnames = classnames\n",
        "        self.n_cls = len(classnames)\n",
        "        self.n_ctx = n_ctx\n",
        "        self.dtype = text_encoder.text_projection.dtype\n",
        "        self.token_embedding = text_encoder.token_embedding          # (vocab_size, width)\n",
        "        self.positional_embedding = text_encoder.positional_embedding\n",
        "        self.transformer = text_encoder.transformer\n",
        "        self.ln_final = text_encoder.ln_final\n",
        "        self.text_projection = text_encoder.text_projection\n",
        "        self.attn_mask = text_encoder.attn_mask                      # may be None for some cfgs\n",
        "        self.vocab_size = self.token_embedding.num_embeddings\n",
        "        self.width = self.token_embedding.embedding_dim\n",
        "        self.register_parameter(\"ctx\", nn.Parameter(torch.randn(n_ctx, self.width, dtype=self.dtype) * 0.02))\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Pre-tokenize classnames once\n",
        "        # Template is: [SOS] prefix, [CTX x n_ctx], class, suffix, [EOS], [PAD...]\n",
        "        prompts = [f\"{prefix} {name}{(' ' + suffix) if suffix else ''}\" for name in classnames]\n",
        "        self.prompts_tokenized = self.tokenizer(prompts)  # (n_cls, context_length)\n",
        "        with torch.no_grad():\n",
        "            # We'll also tokenize the classnames alone to find where they start\n",
        "            self.cls_tokenized = self.tokenizer(classnames)  # helps locate class tokens if needed\n",
        "\n",
        "        # Get CLIP context length\n",
        "        self.context_length = model.context_length\n",
        "\n",
        "    def forward(self):\n",
        "        # Build per-class token embeddings with learned context\n",
        "        prompts = self.prompts_tokenized.to(device)  # (n_cls, context_length)\n",
        "        x = self.token_embedding(prompts).to(self.dtype)  # (n_cls, context_length, width)\n",
        "\n",
        "        # Find the location to insert ctx: right after [SOS] and (prefix) tokens.\n",
        "        # Heuristic: put ctx immediately after the first token (start-of-text, index 0) + the prefix length.\n",
        "        # Compute prefix length from tokenized prefix (minus special tokens).\n",
        "        prefix_tokens = self.tokenizer(self.prefix)\n",
        "        # count non-zero tokens except the starting 0 and trailing 0s\n",
        "        pref_len = int((torch.tensor(prefix_tokens) != 0).sum().item()) - 2  # rough; adjusts ok in practice\n",
        "\n",
        "        # Insert learned context at positions [1 ... n_ctx] after prefix\n",
        "        # NOTE: we’re replacing those slots, not increasing sequence length.\n",
        "        # Ensure we don't overflow context_length.\n",
        "        start = 1 + max(pref_len, 0)\n",
        "        end = min(start + self.n_ctx, self.context_length - 2)  # keep room for [EOS]\n",
        "        ctx = self.ctx[: end - start]                           # (use as many as fit)\n",
        "        x[:, start:end, :] = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        # Standard CLIP text forward pass on our *embedded* tokens\n",
        "        x = x + self.positional_embedding.to(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x, attn_mask=self.attn_mask)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).to(self.dtype)\n",
        "\n",
        "        # Take features at the EOS token (the position of highest token id == tokenizer.eot_token)\n",
        "        # In OpenCLIP, EOT token id is usually tokenizer.eot_token\n",
        "        eot_token = self.tokenizer.eot_token\n",
        "        eot_inds = (self.prompts_tokenized == eot_token).int().argmax(dim=1)\n",
        "        text_embeds = x[torch.arange(x.shape[0]), eot_inds] @ self.text_projection\n",
        "\n",
        "        # L2 normalize for cosine similarities\n",
        "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "        return text_embeds  # (n_cls, d)\n"
      ],
      "id": "F2VS5APMJsBK"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "S5X1HDaJJsBK"
      },
      "outputs": [],
      "source": [
        "class PromptedMobileCLIP(nn.Module):\n",
        "    def __init__(self, base_model, tokenizer, classnames, n_ctx=4):\n",
        "        super().__init__()\n",
        "        self.base = base_model\n",
        "        self.text_encoder = base_model.text\n",
        "        self.image_encoder = base_model.visual\n",
        "\n",
        "        # --- freeze everything in the base, including logit_scale ---\n",
        "        for p in self.base.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        if hasattr(self.base, \"logit_scale\"):\n",
        "            self.base.logit_scale.requires_grad_(False)\n",
        "\n",
        "        self.prompt_learner = PromptLearner(self.text_encoder, tokenizer, classnames, n_ctx)\n",
        "\n",
        "    def forward(self, images):\n",
        "        image_features = self.base.encode_image(images)\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        text_features  = self.prompt_learner()\n",
        "        logit_scale = self.base.logit_scale.exp()\n",
        "        return logit_scale * image_features @ text_features.t()\n"
      ],
      "id": "S5X1HDaJJsBK"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eitGJJ2fJsBL",
        "outputId": "cdd2971f-0fc6-4287-906a-cd4e0b346f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2048\n"
          ]
        }
      ],
      "source": [
        "#model_coop = PromptedMobileCLIP(model, tokenizer, classnames, n_ctx=cfg.prompt_len).to(device)\n",
        "\n",
        "# (Optional) lightly unfreeze some base layers\n",
        "#for name, p in model_coop.base.named_parameters():\n",
        "#    if any(un in name for un in cfg.unfreeze_layers):\n",
        "#        p.requires_grad = True\n",
        "\n",
        "# Optimizer: prompt (and any unfrozen base params) at different LRs\n",
        "#prompt_params = [p for p in model_coop.prompt_learner.parameters() if p.requires_grad]\n",
        "#base_params   = [p for n,p in model_coop.base.named_parameters() if p.requires_grad]\n",
        "\n",
        "#param_groups = []\n",
        "#if base_params:\n",
        "#    param_groups.append({\"params\": base_params, \"lr\": cfg.lr_base, \"weight_decay\": cfg.weight_decay})\n",
        "#param_groups.append({\"params\": prompt_params, \"lr\": cfg.lr_prompt, \"weight_decay\": cfg.weight_decay})\n",
        "\n",
        "model_coop = PromptedMobileCLIP(model, tokenizer, classnames, n_ctx=cfg.prompt_len).to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    [{\"params\": model_coop.prompt_learner.parameters(), \"lr\": cfg.lr_prompt, \"weight_decay\": cfg.weight_decay}]\n",
        ")\n",
        "print(\"Trainable params:\", sum(p.numel() for p in model_coop.parameters() if p.requires_grad))\n"
      ],
      "id": "eitGJJ2fJsBL"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "pehFd223JsBM"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device, loss_fn, desc=\"eval\", pbar=True):\n",
        "    model.eval()\n",
        "    loss_sum, correct, count = 0.0, 0, 0\n",
        "    iterator = tqdm(loader, desc=desc, leave=False) if pbar else loader\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in iterator:\n",
        "            data = data.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "            bs = data.size(0)\n",
        "            loss_sum += loss.item() * bs\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == target).sum().item()\n",
        "            count += bs\n",
        "            if pbar:\n",
        "                acc = (correct / max(1, count)) * 100.0\n",
        "                iterator.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{acc:.2f}%\")\n",
        "\n",
        "    return loss_sum / max(1, count), correct / max(1, count)\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, device, epochs: int,\n",
        "          log_interval: int = 25, ema_alpha: float = 0.1):\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "    model.to(device)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_sum, train_correct, train_count = 0.0, 0, 0\n",
        "        ema = None\n",
        "        iterator = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} | train\", leave=False, dynamic_ncols=True, mininterval=0.5)\n",
        "        for step, (data, target) in enumerate(iterator, start=1):\n",
        "            data = data.to(device, non_blocking=True)\n",
        "            target = target.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            output = model(data)\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # --- DEBUG: grad norms ---\n",
        "            with torch.no_grad():\n",
        "                gnorm_ctx = 0.0\n",
        "                for p in model.prompt_learner.parameters():\n",
        "                    if p.grad is not None:\n",
        "                        g = p.grad.detach()\n",
        "                        gnorm_ctx += (g*g).sum().item()\n",
        "                gnorm_ctx = math.sqrt(gnorm_ctx) if gnorm_ctx > 0 else 0.0\n",
        "                if step % log_interval == 0:\n",
        "                    tqdm.write(f\"[dbg] ctx grad L2 = {gnorm_ctx:.6e}\")\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # stats\n",
        "            bs = data.size(0)\n",
        "            train_sum += (loss.item() * bs)\n",
        "            preds = output.argmax(dim=1)\n",
        "            train_correct += (preds == target).sum().item()\n",
        "            train_count += bs\n",
        "            ema = loss.item() if ema is None else (1 - ema_alpha) * ema + ema_alpha * loss.item()\n",
        "            if (step % log_interval == 0) or (step == len(iterator)):\n",
        "                acc_pct = 100.0 * train_correct / max(1, train_count)\n",
        "                iterator.set_postfix_str(f\"loss(ema)={ema:.4f} acc={acc_pct:.2f}%\")\n",
        "\n",
        "        avg_train_loss = train_sum / max(1, train_count)\n",
        "        avg_train_acc  = train_correct / max(1, train_count)\n",
        "        avg_val_loss, avg_val_acc = evaluate(model, val_loader, device, loss_fn=loss_fn, desc=\"valid\", pbar=True)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['train_acc'].append(avg_train_acc)\n",
        "        history['val_acc'].append(avg_val_acc)\n",
        "\n",
        "        tqdm.write(f\"Epoch {epoch:03d}: train_loss={avg_train_loss:.4f} train_acc={avg_train_acc*100:.2f}%  \"\n",
        "                   f\"val_loss={avg_val_loss:.4f} val_acc={avg_val_acc*100:.2f}%\")\n",
        "\n",
        "    return history\n"
      ],
      "id": "pehFd223JsBM"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nFffgxyGJsBM"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
        "    ax = axes[0]\n",
        "    ax.plot(epochs, history['train_loss'], label='train')\n",
        "    ax.plot(epochs, history['val_loss'], label='val')\n",
        "    ax.set_xlabel('Epoch') ; ax.set_ylabel('Loss') ; ax.set_title('Loss') ; ax.legend()\n",
        "    ax = axes[1]\n",
        "    if 'train_acc' in history and 'val_acc' in history:\n",
        "        ax.plot(epochs, [x * 100 for x in history['train_acc']], label='train')\n",
        "        ax.plot(epochs, [x * 100 for x in history['val_acc']], label='val')\n",
        "        ax.set_ylabel('Accuracy (%)')\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No accuracy in history', ha='center', va='center', transform=ax.transAxes)\n",
        "    ax.set_xlabel('Epoch') ; ax.set_title('Accuracy') ; ax.legend()\n",
        "    plt.show()"
      ],
      "id": "nFffgxyGJsBM"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "7e1d20b7ca1641858c5839ca5db4fd53",
            "e8fde73789824caba87b63a3bbbd22af",
            "86c168a700864623bd606df145d0a784",
            "935d33a032134d15b3841e8d6dc20220",
            "e354032e23b74decbf7d8f627bfe4eb6",
            "ea8dd38ee0a543f9b35c8d0ee201f7f5",
            "60b1e200c9d142f4acf2792312837086",
            "d0a61b5fd6fa40bda0444d721d5d16a9",
            "1656469068ec43e48643b1f5587e4129",
            "15397e84ac16497fb98372684f15f33f",
            "9a588d2eb0fa45a79c9224185244573f"
          ]
        },
        "id": "OWKpT_YhJsBM",
        "outputId": "17aa08b7-b45c-41e3-ea87-e0a481412a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 2048\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/50 | train:   0%|          | 0/1 [00:04<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e1d20b7ca1641858c5839ca5db4fd53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3462586258.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pref_len = int((torch.tensor(prefix_tokens) != 0).sum().item()) - 2  # rough; adjusts ok in practice\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SimpleTokenizer' object has no attribute 'eot_token'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1016767912.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train a bit more for overfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mhistory_tiny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_coop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiny_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiny_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverfit_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_tiny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2951924356.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, device, epochs, log_interval, ema_alpha)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2689498281.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtext_features\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlogit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogit_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3462586258.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Take features at the EOS token (the position of highest token id == tokenizer.eot_token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# In OpenCLIP, EOT token id is usually tokenizer.eot_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0meot_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meot_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0meot_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts_tokenized\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meot_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meot_inds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SimpleTokenizer' object has no attribute 'eot_token'"
          ]
        }
      ],
      "source": [
        "def make_overfit_subset(dataset, n_classes=2, k_per_class=8):\n",
        "    targets = np.array(dataset.targets)  # CIFAR10\n",
        "    selected_indices = []\n",
        "    classes = list(range(len(set(targets))))[:n_classes]\n",
        "    for c in classes:\n",
        "        idx = np.where(targets == c)[0].tolist()\n",
        "        random.shuffle(idx)\n",
        "        selected_indices.extend(idx[:k_per_class])\n",
        "    random.shuffle(selected_indices)\n",
        "    return Subset(dataset, selected_indices)\n",
        "\n",
        "tiny_train = make_overfit_subset(train_ds, cfg.overfit_n_classes, cfg.overfit_k_per_class)\n",
        "tiny_val   = make_overfit_subset(val_ds,   cfg.overfit_n_classes, cfg.overfit_k_per_class)\n",
        "\n",
        "tiny_train_loader = DataLoader(tiny_train, batch_size=min(16, cfg.batch_size), shuffle=True,\n",
        "                               num_workers=1, pin_memory=False, persistent_workers=True)\n",
        "tiny_val_loader   = DataLoader(tiny_val, batch_size=min(32, cfg.batch_size), shuffle=False,\n",
        "                               num_workers=1, pin_memory=False, persistent_workers=True)\n",
        "\n",
        "# quick check: param counts\n",
        "total = sum(p.numel() for p in model_coop.parameters() if p.requires_grad)\n",
        "print(\"Trainable params:\", total)\n",
        "\n",
        "# Train a bit more for overfit\n",
        "history_tiny = train(model_coop, tiny_train_loader, tiny_val_loader, opt, device, epochs=cfg.overfit_epochs)\n",
        "\n",
        "plot_history(history_tiny)\n"
      ],
      "id": "OWKpT_YhJsBM"
    },
    {
      "cell_type": "code",
      "source": [
        "images, y = next(iter(tiny_train_loader))  # or a full loader; any batch is fine\n",
        "images = images.to(device)\n",
        "model_coop.zero_grad(set_to_none=True)\n",
        "logits = model_coop(images)\n",
        "(logits.sum()).backward()\n",
        "print(\"ctx grad L2:\", model_coop.prompt_learner.ctx.grad.norm().item())\n"
      ],
      "metadata": {
        "id": "44Fi8o4RS95z"
      },
      "id": "44Fi8o4RS95z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fd8d1a",
      "metadata": {
        "id": "41fd8d1a"
      },
      "outputs": [],
      "source": [
        "# Rebuild optimizer if you want to reset after overfit\n",
        "opt_full = torch.optim.AdamW(\n",
        "    [{\"params\": [p for n,p in model_coop.base.named_parameters() if p.requires_grad], \"lr\": cfg.lr_base,   \"weight_decay\": cfg.weight_decay},\n",
        "     {\"params\": [p for p in model_coop.prompt_learner.parameters()],                 \"lr\": cfg.lr_prompt, \"weight_decay\": cfg.weight_decay}]\n",
        ")\n",
        "history = train(model_coop, train_loader, val_loader, opt, device, epochs=cfg.max_epochs)\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uudZvBs3JsBN"
      },
      "outputs": [],
      "source": [
        "# 10) Extra analysis: (optional) confusion matrix if sklearn is available\n",
        "try:\n",
        "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(test_loader, leave=False, desc='cm'):\n",
        "            p = model_coop(x.to(device)).argmax(dim=1).cpu()\n",
        "            y_true.extend(y.cpu().tolist())\n",
        "            y_pred.extend(p.tolist())\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=classnames)\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    disp.plot(ax=ax, include_values=False, xticks_rotation=90, cmap='Blues')\n",
        "    plt.title('Confusion Matrix (test)')\n",
        "    plt.tight_layout(); plt.show()\n",
        "except Exception as e:\n",
        "    print('Install scikit-learn to see confusion matrix. Skipping. Error:', e)"
      ],
      "id": "uudZvBs3JsBN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLPQqMGfJsBN"
      },
      "source": [
        "### Notes\n",
        "- The backbone is **frozen**; only the prompt parameters `ctx` (shape `[M, D]`) are trained.\n",
        "- For MobileCLIP(2), keeping the model in **eval()** avoids BN updates.\n",
        "- You can change `cfg.prompt_len` to explore different context lengths (e.g., 4, 8, 16).\n",
        "- To compare with classic prompts, see `zeroshot_classifier()` helper."
      ],
      "id": "WLPQqMGfJsBN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e1d20b7ca1641858c5839ca5db4fd53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8fde73789824caba87b63a3bbbd22af",
              "IPY_MODEL_86c168a700864623bd606df145d0a784",
              "IPY_MODEL_935d33a032134d15b3841e8d6dc20220"
            ],
            "layout": "IPY_MODEL_e354032e23b74decbf7d8f627bfe4eb6"
          }
        },
        "e8fde73789824caba87b63a3bbbd22af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea8dd38ee0a543f9b35c8d0ee201f7f5",
            "placeholder": "​",
            "style": "IPY_MODEL_60b1e200c9d142f4acf2792312837086",
            "value": "Epoch 1/50 | train:   0%"
          }
        },
        "86c168a700864623bd606df145d0a784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0a61b5fd6fa40bda0444d721d5d16a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1656469068ec43e48643b1f5587e4129",
            "value": 0
          }
        },
        "935d33a032134d15b3841e8d6dc20220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15397e84ac16497fb98372684f15f33f",
            "placeholder": "​",
            "style": "IPY_MODEL_9a588d2eb0fa45a79c9224185244573f",
            "value": " 0/1 [00:04&lt;?, ?it/s]"
          }
        },
        "e354032e23b74decbf7d8f627bfe4eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "ea8dd38ee0a543f9b35c8d0ee201f7f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b1e200c9d142f4acf2792312837086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0a61b5fd6fa40bda0444d721d5d16a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1656469068ec43e48643b1f5587e4129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15397e84ac16497fb98372684f15f33f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a588d2eb0fa45a79c9224185244573f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}