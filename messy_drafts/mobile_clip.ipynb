{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d449e9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open_clip_torch in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (0.23.0)\n",
      "Requirement already satisfied: regex in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (2025.7.34)\n",
      "Requirement already satisfied: ftfy in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (0.34.4)\n",
      "Requirement already satisfied: safetensors in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from open_clip_torch) (1.0.19)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from timm>=1.0.17->open_clip_torch) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torch>=2.0->open_clip_torch) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub->open_clip_torch) (2.32.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from tqdm->open_clip_torch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->open_clip_torch) (2025.8.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchvision->open_clip_torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vlad\\miniconda3\\envs\\nlp\\lib\\site-packages (from torchvision->open_clip_torch) (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85e42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RN50', 'openai'),\n",
      " ('RN50', 'yfcc15m'),\n",
      " ('RN50', 'cc12m'),\n",
      " ('RN101', 'openai'),\n",
      " ('RN101', 'yfcc15m'),\n",
      " ('RN50x4', 'openai'),\n",
      " ('RN50x16', 'openai'),\n",
      " ('RN50x64', 'openai'),\n",
      " ('ViT-B-32', 'openai'),\n",
      " ('ViT-B-32', 'laion400m_e31'),\n",
      " ('ViT-B-32', 'laion400m_e32'),\n",
      " ('ViT-B-32', 'laion2b_e16'),\n",
      " ('ViT-B-32', 'laion2b_s34b_b79k'),\n",
      " ('ViT-B-32', 'datacomp_xl_s13b_b90k'),\n",
      " ('ViT-B-32', 'datacomp_m_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_clip_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_laion_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_image_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_text_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_basic_s128m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_m_s128m_b4k'),\n",
      " ('ViT-B-32', 'datacomp_s_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_clip_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_laion_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_image_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_text_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_basic_s13m_b4k'),\n",
      " ('ViT-B-32', 'commonpool_s_s13m_b4k'),\n",
      " ('ViT-B-32', 'metaclip_400m'),\n",
      " ('ViT-B-32', 'metaclip_fullcc'),\n",
      " ('ViT-B-32-256', 'datacomp_s34b_b86k'),\n",
      " ('ViT-B-16', 'openai'),\n",
      " ('ViT-B-16', 'laion400m_e31'),\n",
      " ('ViT-B-16', 'laion400m_e32'),\n",
      " ('ViT-B-16', 'laion2b_s34b_b88k'),\n",
      " ('ViT-B-16', 'datacomp_xl_s13b_b90k'),\n",
      " ('ViT-B-16', 'datacomp_l_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_clip_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_laion_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_image_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_text_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_basic_s1b_b8k'),\n",
      " ('ViT-B-16', 'commonpool_l_s1b_b8k'),\n",
      " ('ViT-B-16', 'dfn2b'),\n",
      " ('ViT-B-16', 'metaclip_400m'),\n",
      " ('ViT-B-16', 'metaclip_fullcc'),\n",
      " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
      " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
      " ('ViT-L-14', 'openai'),\n",
      " ('ViT-L-14', 'laion400m_e31'),\n",
      " ('ViT-L-14', 'laion400m_e32'),\n",
      " ('ViT-L-14', 'laion2b_s32b_b82k'),\n",
      " ('ViT-L-14', 'datacomp_xl_s13b_b90k'),\n",
      " ('ViT-L-14', 'commonpool_xl_clip_s13b_b90k'),\n",
      " ('ViT-L-14', 'commonpool_xl_laion_s13b_b90k'),\n",
      " ('ViT-L-14', 'commonpool_xl_s13b_b90k'),\n",
      " ('ViT-L-14', 'metaclip_400m'),\n",
      " ('ViT-L-14', 'metaclip_fullcc'),\n",
      " ('ViT-L-14', 'dfn2b'),\n",
      " ('ViT-L-14', 'dfn2b_s39b'),\n",
      " ('ViT-L-14-336', 'openai'),\n",
      " ('ViT-H-14', 'laion2b_s32b_b79k'),\n",
      " ('ViT-H-14', 'metaclip_fullcc'),\n",
      " ('ViT-H-14', 'metaclip_altogether'),\n",
      " ('ViT-H-14', 'dfn5b'),\n",
      " ('ViT-H-14-378', 'dfn5b'),\n",
      " ('ViT-g-14', 'laion2b_s12b_b42k'),\n",
      " ('ViT-g-14', 'laion2b_s34b_b88k'),\n",
      " ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n",
      " ('ViT-bigG-14', 'metaclip_fullcc'),\n",
      " ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n",
      " ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n",
      " ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n",
      " ('convnext_base', 'laion400m_s13b_b51k'),\n",
      " ('convnext_base_w', 'laion2b_s13b_b82k'),\n",
      " ('convnext_base_w', 'laion2b_s13b_b82k_augreg'),\n",
      " ('convnext_base_w', 'laion_aesthetic_s13b_b82k'),\n",
      " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k'),\n",
      " ('convnext_base_w_320', 'laion_aesthetic_s13b_b82k_augreg'),\n",
      " ('convnext_large_d', 'laion2b_s26b_b102k_augreg'),\n",
      " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft'),\n",
      " ('convnext_large_d_320', 'laion2b_s29b_b131k_ft_soup'),\n",
      " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg'),\n",
      " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_rewind'),\n",
      " ('convnext_xxlarge', 'laion2b_s34b_b82k_augreg_soup'),\n",
      " ('coca_ViT-B-32', 'laion2b_s13b_b90k'),\n",
      " ('coca_ViT-B-32', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
      " ('coca_ViT-L-14', 'laion2b_s13b_b90k'),\n",
      " ('coca_ViT-L-14', 'mscoco_finetuned_laion2b_s13b_b90k'),\n",
      " ('EVA01-g-14', 'laion400m_s11b_b41k'),\n",
      " ('EVA01-g-14-plus', 'merged2b_s11b_b114k'),\n",
      " ('EVA02-B-16', 'merged2b_s8b_b131k'),\n",
      " ('EVA02-L-14', 'merged2b_s4b_b131k'),\n",
      " ('EVA02-L-14-336', 'merged2b_s6b_b61k'),\n",
      " ('EVA02-E-14', 'laion2b_s4b_b115k'),\n",
      " ('EVA02-E-14-plus', 'laion2b_s9b_b144k'),\n",
      " ('ViT-B-16-SigLIP', 'webli'),\n",
      " ('ViT-B-16-SigLIP-256', 'webli'),\n",
      " ('ViT-B-16-SigLIP-i18n-256', 'webli'),\n",
      " ('ViT-B-16-SigLIP-384', 'webli'),\n",
      " ('ViT-B-16-SigLIP-512', 'webli'),\n",
      " ('ViT-L-16-SigLIP-256', 'webli'),\n",
      " ('ViT-L-16-SigLIP-384', 'webli'),\n",
      " ('ViT-SO400M-14-SigLIP', 'webli'),\n",
      " ('ViT-SO400M-16-SigLIP-i18n-256', 'webli'),\n",
      " ('ViT-SO400M-14-SigLIP-378', 'webli'),\n",
      " ('ViT-SO400M-14-SigLIP-384', 'webli'),\n",
      " ('ViT-B-32-SigLIP2-256', 'webli'),\n",
      " ('ViT-B-16-SigLIP2', 'webli'),\n",
      " ('ViT-B-16-SigLIP2-256', 'webli'),\n",
      " ('ViT-B-16-SigLIP2-384', 'webli'),\n",
      " ('ViT-B-16-SigLIP2-512', 'webli'),\n",
      " ('ViT-L-16-SigLIP2-256', 'webli'),\n",
      " ('ViT-L-16-SigLIP2-384', 'webli'),\n",
      " ('ViT-L-16-SigLIP2-512', 'webli'),\n",
      " ('ViT-SO400M-14-SigLIP2', 'webli'),\n",
      " ('ViT-SO400M-14-SigLIP2-378', 'webli'),\n",
      " ('ViT-SO400M-16-SigLIP2-256', 'webli'),\n",
      " ('ViT-SO400M-16-SigLIP2-384', 'webli'),\n",
      " ('ViT-SO400M-16-SigLIP2-512', 'webli'),\n",
      " ('ViT-gopt-16-SigLIP2-256', 'webli'),\n",
      " ('ViT-gopt-16-SigLIP2-384', 'webli'),\n",
      " ('ViT-L-14-CLIPA', 'datacomp1b'),\n",
      " ('ViT-L-14-CLIPA-336', 'datacomp1b'),\n",
      " ('ViT-H-14-CLIPA', 'datacomp1b'),\n",
      " ('ViT-H-14-CLIPA-336', 'laion2b'),\n",
      " ('ViT-H-14-CLIPA-336', 'datacomp1b'),\n",
      " ('ViT-bigG-14-CLIPA', 'datacomp1b'),\n",
      " ('ViT-bigG-14-CLIPA-336', 'datacomp1b'),\n",
      " ('nllb-clip-base', 'v1'),\n",
      " ('nllb-clip-large', 'v1'),\n",
      " ('nllb-clip-base-siglip', 'v1'),\n",
      " ('nllb-clip-base-siglip', 'mrl'),\n",
      " ('nllb-clip-large-siglip', 'v1'),\n",
      " ('nllb-clip-large-siglip', 'mrl'),\n",
      " ('MobileCLIP-S1', 'datacompdr'),\n",
      " ('MobileCLIP-S2', 'datacompdr'),\n",
      " ('MobileCLIP-B', 'datacompdr'),\n",
      " ('MobileCLIP-B', 'datacompdr_lt'),\n",
      " ('MobileCLIP2-B', 'dfndr2b'),\n",
      " ('MobileCLIP2-S0', 'dfndr2b'),\n",
      " ('MobileCLIP2-S2', 'dfndr2b'),\n",
      " ('MobileCLIP2-S3', 'dfndr2b'),\n",
      " ('MobileCLIP2-S4', 'dfndr2b'),\n",
      " ('MobileCLIP2-L-14', 'dfndr2b'),\n",
      " ('ViTamin-S', 'datacomp1b'),\n",
      " ('ViTamin-S-LTT', 'datacomp1b'),\n",
      " ('ViTamin-B', 'datacomp1b'),\n",
      " ('ViTamin-B-LTT', 'datacomp1b'),\n",
      " ('ViTamin-L', 'datacomp1b'),\n",
      " ('ViTamin-L-256', 'datacomp1b'),\n",
      " ('ViTamin-L-336', 'datacomp1b'),\n",
      " ('ViTamin-L-384', 'datacomp1b'),\n",
      " ('ViTamin-L2', 'datacomp1b'),\n",
      " ('ViTamin-L2-256', 'datacomp1b'),\n",
      " ('ViTamin-L2-336', 'datacomp1b'),\n",
      " ('ViTamin-L2-384', 'datacomp1b'),\n",
      " ('ViTamin-XL-256', 'datacomp1b'),\n",
      " ('ViTamin-XL-336', 'datacomp1b'),\n",
      " ('ViTamin-XL-384', 'datacomp1b'),\n",
      " ('PE-Core-T-16-384', 'meta'),\n",
      " ('PE-Core-S-16-384', 'meta'),\n",
      " ('PE-Core-B-16', 'meta'),\n",
      " ('PE-Core-L-14-336', 'meta'),\n",
      " ('PE-Core-bigG-14-448', 'meta'),\n",
      " ('ViT-H-14-worldwide', 'metaclip2_worldwide'),\n",
      " ('ViT-H-14-worldwide-378', 'metaclip2_worldwide'),\n",
      " ('ViT-bigG-14-worldwide', 'metaclip2_worldwide'),\n",
      " ('ViT-bigG-14-worldwide-378', 'metaclip2_worldwide'),\n",
      " ('RN50-quickgelu', 'openai'),\n",
      " ('RN50-quickgelu', 'yfcc15m'),\n",
      " ('RN50-quickgelu', 'cc12m'),\n",
      " ('RN101-quickgelu', 'openai'),\n",
      " ('RN101-quickgelu', 'yfcc15m'),\n",
      " ('RN50x4-quickgelu', 'openai'),\n",
      " ('RN50x16-quickgelu', 'openai'),\n",
      " ('RN50x64-quickgelu', 'openai'),\n",
      " ('ViT-B-32-quickgelu', 'openai'),\n",
      " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
      " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
      " ('ViT-B-32-quickgelu', 'metaclip_400m'),\n",
      " ('ViT-B-32-quickgelu', 'metaclip_fullcc'),\n",
      " ('ViT-B-16-quickgelu', 'openai'),\n",
      " ('ViT-B-16-quickgelu', 'dfn2b'),\n",
      " ('ViT-B-16-quickgelu', 'metaclip_400m'),\n",
      " ('ViT-B-16-quickgelu', 'metaclip_fullcc'),\n",
      " ('ViT-L-14-quickgelu', 'openai'),\n",
      " ('ViT-L-14-quickgelu', 'metaclip_400m'),\n",
      " ('ViT-L-14-quickgelu', 'metaclip_fullcc'),\n",
      " ('ViT-L-14-quickgelu', 'dfn2b'),\n",
      " ('ViT-L-14-336-quickgelu', 'openai'),\n",
      " ('ViT-H-14-quickgelu', 'metaclip_fullcc'),\n",
      " ('ViT-H-14-quickgelu', 'dfn5b'),\n",
      " ('ViT-H-14-378-quickgelu', 'dfn5b'),\n",
      " ('ViT-bigG-14-quickgelu', 'metaclip_fullcc'),\n",
      " ('ViT-H-14-worldwide-quickgelu', 'metaclip2_worldwide')]\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "import pprint\n",
    "pprint.pp([p for p in open_clip.list_pretrained()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156215cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import open_clip\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c0fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomTextCLIP(\n",
      "  (visual): TimmModel(\n",
      "    (trunk): VisionTransformer(\n",
      "      (patch_embed): HybridEmbed(\n",
      "        (backbone): ConvStem(\n",
      "          (0): ConvNormAct(\n",
      "            (conv): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): GELU(approximate='none')\n",
      "            )\n",
      "          )\n",
      "          (1): ConvNormAct(\n",
      "            (conv): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): GELU(approximate='none')\n",
      "            )\n",
      "          )\n",
      "          (2): ConvNormAct(\n",
      "            (conv): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "            (bn): Sequential()\n",
      "          )\n",
      "        )\n",
      "        (proj): Identity()\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (patch_drop): Identity()\n",
      "      (norm_pre): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (3): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (4): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (5): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (6): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (7): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (8): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (9): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (10): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (11): Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (q_norm): Identity()\n",
      "            (k_norm): Identity()\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (fc_norm): Identity()\n",
      "      (head_drop): Dropout(p=0.0, inplace=False)\n",
      "      (head): Linear(in_features=768, out_features=512, bias=True)\n",
      "    )\n",
      "    (head): Sequential()\n",
      "  )\n",
      "  (text): TextTransformer(\n",
      "    (token_embedding): Embedding(49408, 512)\n",
      "    (transformer): Transformer(\n",
      "      (resblocks): ModuleList(\n",
      "        (0-11): 12 x ResidualAttentionBlock(\n",
      "          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (ls_1): Identity()\n",
      "          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (gelu): GELU(approximate='none')\n",
      "            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          )\n",
      "          (ls_2): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"MobileCLIP2-B\"\n",
    "PRETRAINED = \"dfndr2b\"\n",
    "\n",
    "base_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)\n",
    "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
    "print(base_model)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d6b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOpPromptClassifier(nn.Module):\n",
    "    def __init__(self, base_model, tokenizer, classnames, n_ctx=16, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classnames = classnames\n",
    "        self.n_ctx = n_ctx\n",
    "        self.device = device\n",
    "\n",
    "        # Make it work for any CLIP-like model\n",
    "        if hasattr(self.base_model, 'text'):\n",
    "            self.text_tower = self.base_model.text\n",
    "        else:\n",
    "            self.text_tower = self.base_model\n",
    "        if hasattr(self.base_model, 'visual'):\n",
    "            self.visual_tower = self.base_model.visual\n",
    "        else:\n",
    "            self.visual_tower = self.base_model\n",
    "\n",
    "        self.embed_size = self.text_tower.token_embedding.shape[1]\n",
    "        self.ctx = nn.Parameter(torch.randn(n_ctx, self.embed_size))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.class_token_ids = self.tokenizer(classnames).to(self.device)\n",
    "            self.class_token_embeddings = self.text_tower.token_embedding(self.class_token_ids)\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward_text_features(self):\n",
    "        torch.cat([self.ctx, self.base_model.text.token_embedding(self.class_token_ids)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOpPromptRetrieval(nn.Module):\n",
    "    def __init__(self, base_model, tokenizer, n_ctx=16, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_ctx = n_ctx\n",
    "        self.device = device\n",
    "\n",
    "        # Make it work for any CLIP-like model\n",
    "        if hasattr(self.base_model, 'text'):\n",
    "            self.text_tower = self.base_model.text\n",
    "        else:\n",
    "            self.text_tower = self.base_model\n",
    "        if hasattr(self.base_model, 'visual'):\n",
    "            self.visual_tower = self.base_model.visual\n",
    "        else:\n",
    "            self.visual_tower = self.base_model\n",
    "\n",
    "        self.embedding_size = self.text_tower.token_embedding.weight.shape[1]\n",
    "        self.ctx = nn.Parameter(0.02 * torch.randn(n_ctx, self.embedding_size))\n",
    "\n",
    "        for p in self.base_model.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def embeddings(self, text):\n",
    "        return self.text_tower.token_embedding(self.tokenizer(text).to(self.device))\n",
    "\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        # Tokenize if needed\n",
    "        if isinstance(text, list) or (isinstance(text, torch.Tensor) and text.dtype == torch.object):\n",
    "            tokens = self.tokenizer(text).to(self.device)          # [B, L]\n",
    "        else:\n",
    "            tokens = text.to(self.device)                          # already token ids: [B, L]\n",
    "\n",
    "        B, L = tokens.shape\n",
    "\n",
    "        # Text: inject learned context after BOS (position 0)\n",
    "        tok = self.text_tower.token_embedding(tokens)              # [B, L, D]\n",
    "        x = tok\n",
    "        n = min(self.n_ctx, L - 2)                                 # keep BOS/EOS\n",
    "        if n > 0:\n",
    "            x = x.clone()\n",
    "            x[:, 1:1 + n, :] = self.ctx[:n].unsqueeze(0).expand(B, n, -1)\n",
    "\n",
    "        x = x + self.text_tower.positional_embedding[:L].unsqueeze(0)\n",
    "        x = x.permute(1, 0, 2)                                     # [L, B, D]\n",
    "        x = self.text_tower.transformer(x)\n",
    "        x = x.permute(1, 0, 2)                                     # [B, L, D]\n",
    "        x = self.text_tower.ln_final(x)\n",
    "\n",
    "        eos_idx = tokens.argmax(dim=-1)                            # EOS position\n",
    "        text_features = x[torch.arange(B, device=self.device), eos_idx]\n",
    "        text_features = text_features @ self.text_tower.text_projection\n",
    "\n",
    "        # Image: standard encode\n",
    "        image = image.to(self.device)\n",
    "        image_features = self.base_model.encode_image(image)\n",
    "\n",
    "        return image_features, text_features\n",
    "\n",
    "    \n",
    "    def logits(self, image, text):\n",
    "        image_features, text_features = self.forward(image, text)\n",
    "        logit_scale = self.base_model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features.T\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338947ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "coop_retrieval_model = CoOpPromptRetrieval(base_model, tokenizer, n_ctx=16, device=device)\n",
    "optimizer = torch.optim.AdamW([coop_retrieval_model.ctx], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf07cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_ds = load_dataset(\"AnyModal/flickr30k\", split=\"train\")\n",
    "val_ds = load_dataset(\"AnyModal/flickr30k\", split=\"validation\")\n",
    "\n",
    "train_ds = train_ds.with_transform(\n",
    "    lambda ex: {\n",
    "        \"image\":   [preprocess_train(img) for img in ex[\"image\"]],\n",
    "        \"alt_text\":[caps[0] for caps in ex[\"alt_text\"]],\n",
    "    }\n",
    ")\n",
    "val_ds = val_ds.with_transform(\n",
    "    lambda ex: {\n",
    "        \"image\":   [preprocess_val(img) for img in ex[\"image\"]],\n",
    "        \"alt_text\":[caps[0] for caps in ex[\"alt_text\"]],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77064af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, val_loss 173.32937399358204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, val_loss 173.87181479549972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, val_loss 174.71024691188595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, val_loss 175.717343422553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, val_loss 176.7060768082297\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import get_data, fit_retrieval, clip_contrastive_loss_from_logits\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dl, val_dl = get_data(train_ds, val_ds, bs=BATCH_SIZE, n_shot=8)\n",
    "fit_retrieval(EPOCHS, coop_retrieval_model, clip_contrastive_loss_from_logits, optimizer, train_dl, val_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
