{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "-2NbWPlKLK4Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2NbWPlKLK4Q",
        "outputId": "4502503c-3fd4-43ae-f931-4143adf961f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIf you need packages, uncomment the pip lines above and run this cell.\n"
          ]
        }
      ],
      "source": [
        "# 1) (Optional) Installs — uncomment on first run\n",
        "# !pip install -U matplotlib timm torchvision torch tqdm scikit-learn --quiet\n",
        "!pip install -U open-clip-torch torchinfo --quiet\n",
        "print('If you need packages, uncomment the pip lines above and run this cell.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"         # make errors point to the right op\n",
        "#os.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"   # show C++ stack"
      ],
      "metadata": {
        "id": "zNl73r0Aa1rr"
      },
      "id": "zNl73r0Aa1rr",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4B8-K5xhgjCh",
      "metadata": {
        "id": "4B8-K5xhgjCh"
      },
      "outputs": [],
      "source": [
        "# 2) Common imports + seeds\n",
        "import math, random, os, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import torchinfo\n",
        "\n",
        "import datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import open_clip\n",
        "\n",
        "def set_seed(seed:int=42):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    #torch.backends.cudnn.deterministic = True\n",
        "    #torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(123)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Device is {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XPUNz2engqAX",
      "metadata": {
        "id": "XPUNz2engqAX"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "# Pick a lightweight variant for quick experiments\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    #model_name: str = \"ViT-B-32\"  # new MobileCLIP2 in open_clip\n",
        "    #pretrained: str = \"laion2b_s34b_b79k\"\n",
        "    model_name = 'MobileCLIP2-S0'   # alternatives: 'MobileCLIP2-S2', 'MobileCLIP2-B', 'MobileCLIP2-S3', 'MobileCLIP2-S4', 'MobileCLIP2-L-14'\n",
        "    pretrained = 'dfndr2b'          # per MobileCLIP2 release in OpenCLIP\n",
        "\n",
        "    dataset = 'AnyModal/flickr30k'\n",
        "\n",
        "    coop_prompt_save_path = 's0_coop_flickr30k.pt'\n",
        "\n",
        "    image_size: int = 224\n",
        "    batch_size: int = 128\n",
        "    num_workers: int = 4\n",
        "    max_epochs: int = 3\n",
        "\n",
        "    # LR split: base vs prompt\n",
        "    lr_base: float = 1e-5\n",
        "    lr_prompt: float = 1e-4\n",
        "    weight_decay: float = 0.05\n",
        "\n",
        "    # Unfreeze (if you want a light finetune on top of prompt)\n",
        "    unfreeze_layers: Tuple[str, ...] = tuple()  # e.g. (\"visual.transformer.resblocks.11\",)\n",
        "\n",
        "    # CoOp prompt length\n",
        "    prompt_len: int = 16\n",
        "    #k_per_class: int = 8\n",
        "\n",
        "    log_train_interval: int = 25\n",
        "    log_val_interval: int = 50\n",
        "\n",
        "    # Overfit sanity tiny subset\n",
        "    #overfit_n_classes: int = 2\n",
        "    #overfit_k_per_class: int = 8\n",
        "    #overfit_epochs: int = 50\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11fi4S_hLO8",
      "metadata": {
        "id": "e11fi4S_hLO8"
      },
      "outputs": [],
      "source": [
        "# --- Load model + preprocess ---\n",
        "clip_model, _, preprocess = open_clip.create_model_and_transforms(cfg.model_name, pretrained=cfg.pretrained)\n",
        "tokenizer = open_clip.get_tokenizer(cfg.model_name)\n",
        "\n",
        "torchinfo.summary(clip_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QgRwc1P5VtSM",
      "metadata": {
        "id": "QgRwc1P5VtSM"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "full_ds = datasets.load_dataset(\n",
        "    cfg.dataset,\n",
        "    cache_dir=\"./hf_data\",\n",
        ")\n",
        "\n",
        "train_ds = full_ds[\"train\"]\n",
        "val_ds = full_ds[\"validation\"]\n",
        "#test_ds = full_ds[\"test\"]\n",
        "\n",
        "def transform(batch):\n",
        "    imgs = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
        "    batch[\"pixel_values\"] = [preprocess(img) for img in imgs]\n",
        "    return batch\n",
        "\n",
        "train_ds = train_ds.with_transform(transform)\n",
        "val_ds   = val_ds.with_transform(transform)\n",
        "#test_ds = test_ds.with_transform(transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([b[\"pixel_values\"] for b in batch])  # [B,3,H,W]\n",
        "    descriptions = [b[\"alt_text\"][0] for b in batch]\n",
        "    description_ids = tokenizer(descriptions)\n",
        "    #image_features = clip_model.encode_image(images)\n",
        "    #image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    return {\n",
        "        \"pixel_values\": images,\n",
        "        #\"image_features\": image_features, # Can be precomputed as the image tower is frozen\n",
        "        \"description\": descriptions,\n",
        "        \"description_ids\": description_ids,\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "#train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Val dataset:   {len(val_ds)} images with descriptions\")\n",
        "\n",
        "def show_batch(dl: DataLoader, rows: int=2, cols: int=8):\n",
        "    batch = next(iter(dl))\n",
        "    images, descriptions = batch[\"pixel_values\"], batch[\"description\"]\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            idx = i*cols + j\n",
        "            if idx >= len(images):\n",
        "                break\n",
        "            img = images[idx].permute(1, 2, 0).cpu().numpy()\n",
        "            desc = descriptions[idx]\n",
        "            axs[i, j].imshow(img)\n",
        "            axs[i, j].set_title(desc)\n",
        "            axs[i, j].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_batch(val_loader, rows=4, cols=2)"
      ],
      "metadata": {
        "id": "SwTstWwcQdSq"
      },
      "id": "SwTstWwcQdSq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load classification dataset (flower102)"
      ],
      "metadata": {
        "id": "TAANmaXuR_UQ"
      },
      "id": "TAANmaXuR_UQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "cls_full_ds = datasets.load_dataset(\n",
        "    \"jonathan-roberts1/EuroSAT\",\n",
        "    cache_dir=\"./hf_data\",\n",
        ")\n",
        "cls_full_train_ds = cls_full_ds[\"train\"]\n",
        "cls_splits = cls_full_train_ds.train_test_split(test_size=0.1)\n",
        "cls_full_ds = DatasetDict({\n",
        "    \"train\": cls_splits[\"train\"],\n",
        "    \"validation\": cls_splits[\"test\"]\n",
        "})\n",
        "cls_train_ds = cls_full_ds[\"train\"]\n",
        "cls_val_ds = cls_full_ds[\"validation\"]\n",
        "#test_ds = full_ds[\"test\"]\n",
        "\n",
        "def transform(batch):\n",
        "    imgs = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
        "    batch[\"pixel_values\"] = [preprocess(img) for img in imgs]\n",
        "    return batch\n",
        "\n",
        "cls_train_ds = cls_train_ds.with_transform(transform)\n",
        "cls_val_ds   = cls_val_ds.with_transform(transform)\n",
        "#test_ds = test_ds.with_transform(transform)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([b[\"pixel_values\"] for b in batch])  # [B,3,H,W]\n",
        "    labels = torch.tensor([b[\"label\"] for b in batch])       # [B]\n",
        "    return images, labels\n",
        "\n",
        "cls_train_loader = DataLoader(cls_train_ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "cls_val_loader = DataLoader(cls_val_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "#train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(cls_train_ds.features[\"label\"])\n",
        "cls_label_feature = cls_train_ds.features[\"label\"]\n",
        "classnames = cls_label_feature.names\n",
        "num_classes = cls_label_feature.num_classes\n",
        "\n",
        "print(classnames)"
      ],
      "metadata": {
        "id": "cEx475s4R_Ap"
      },
      "id": "cEx475s4R_Ap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9032dbcf",
      "metadata": {
        "id": "9032dbcf"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_fewshot_dataloader(dl, k, *, label_col=\"label\", seed=42):\n",
        "    ds = dl.dataset\n",
        "    #orig_tf = getattr(ds, \"_transform\", None)          # remember HF transform (if any)\n",
        "    #base = ds.with_transform(None)                     # disable during index selection\n",
        "    print(ds)\n",
        "\n",
        "    # collect indices per class (HF: fast column access)\n",
        "    idx_by_label = defaultdict(list)\n",
        "    for i, batch in enumerate(ds):\n",
        "        y = batch[label_col]\n",
        "        idx_by_label[y].append(i)\n",
        "\n",
        "    # choose up to k per class, then shuffle globally\n",
        "    rng = random.Random(seed)\n",
        "    selected = []\n",
        "    for idxs in idx_by_label.values():\n",
        "        rng.shuffle(idxs)\n",
        "        selected.extend(idxs[:k])\n",
        "    rng.shuffle(selected)\n",
        "\n",
        "    subset = ds.select(selected)\n",
        "    #if orig_tf is not None:\n",
        "    #    subset = subset.with_transform(orig_tf)\n",
        "\n",
        "    # rebuild a DataLoader reusing the original settings\n",
        "    return DataLoader(\n",
        "        subset,\n",
        "        batch_size=dl.batch_size or 1,\n",
        "        shuffle=False,                 # we already shuffled indices\n",
        "        collate_fn=dl.collate_fn,\n",
        "        num_workers=dl.num_workers,\n",
        "        pin_memory=getattr(dl, \"pin_memory\", False),\n",
        "        drop_last=getattr(dl, \"drop_last\", False),\n",
        "        persistent_workers=getattr(dl, \"persistent_workers\", False),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a428dd",
      "metadata": {
        "id": "25a428dd"
      },
      "outputs": [],
      "source": [
        "class CoOpPrompt(nn.Module):\n",
        "    def __init__(self, clip, tokenizer, n_ctx=16, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.clip = clip\n",
        "        self.text_tower = getattr(self.clip, \"text\", self.clip)\n",
        "        #self.visual_tower = getattr(self.model, \"visual\", self.model)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Text transformer sizes\n",
        "        self.context_length = getattr(self.text_tower, \"context_length\", 77)\n",
        "        self.width = self.text_tower.token_embedding.weight.shape[1] # 512\n",
        "\n",
        "        # Learnable context (n_ctx continuous \"tokens\")\n",
        "        self.ctx = nn.Parameter(init_scale * torch.randn(n_ctx, self.width))\n",
        "\n",
        "        # Freeze CLIP\n",
        "        for p in self.text_tower.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.clip.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if mode:\n",
        "          self.clip.eval()\n",
        "          self.text_tower.train()\n",
        "        else:\n",
        "          self.clip.eval()\n",
        "          self.text_tower.eval()\n",
        "\n",
        "    def forward(self, text_ids):\n",
        "        #print(text_ids[0])\n",
        "        eot_indices = text_ids.argmax(dim=-1)\n",
        "        # text_ids: [B, L]\n",
        "        B = text_ids.size(0)\n",
        "\n",
        "        # Frozen lookup for class tokens\n",
        "        with torch.no_grad():\n",
        "            tok_emb = self.text_tower.token_embedding(text_ids)  # [B, L, W]\n",
        "\n",
        "        sos = tok_emb[:, :1, :]         # [B,1,W]\n",
        "        description_part = tok_emb[:, 1:, :]  # [B,L-1,W]\n",
        "\n",
        "        # Learnable context repeated across classes\n",
        "        ctx = self.ctx.unsqueeze(0).expand(B, -1, -1)  # [B, n_ctx, W]\n",
        "\n",
        "        # [SOS] + [CTX...CTX] + [description tokens...]\n",
        "        x = torch.cat([sos, ctx, description_part], dim=1)  # [B, 1+n_ctx+(L-1), W]\n",
        "\n",
        "        # Pad/truncate to model context length\n",
        "        L_target = getattr(self.text_tower, \"context_length\", 77)\n",
        "        if x.size(1) > L_target:\n",
        "            x = x[:, :L_target, :]\n",
        "        elif x.size(1) < L_target:\n",
        "            pad_len = L_target - x.size(1)\n",
        "            pad = x.new_zeros(B, pad_len, x.size(2))\n",
        "            x = torch.cat([x, pad], dim=1)\n",
        "        L = x.size(1)\n",
        "\n",
        "        # Dtypes, pos emb, and (optional) attn mask, all sliced to L\n",
        "        text_dtype = self.text_tower.token_embedding.weight.dtype\n",
        "        pos = self.text_tower.positional_embedding[:L].to(text_dtype)  # [L,W]\n",
        "\n",
        "        attn = getattr(self.text_tower, \"attn_mask\", None)\n",
        "        if attn is not None:\n",
        "            attn = attn[:L, :L].to(text_dtype)  # ensure (L,L)\n",
        "\n",
        "        # Respect transformer.batch_first\n",
        "        batch_first = getattr(self.text_tower.transformer, \"batch_first\", False)\n",
        "        if batch_first:\n",
        "            # want [N, L, D]\n",
        "            x = x.to(text_dtype) + pos.unsqueeze(0)   # [C,L,W]\n",
        "            x = self.text_tower.transformer(x, attn_mask=attn)  # stays [C,L,W]\n",
        "            x = self.text_tower.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "        else:\n",
        "            # want [L, N, D]\n",
        "            x = (x.to(text_dtype) + pos).permute(1, 0, 2)  # [L,C,W]\n",
        "            x = self.text_tower.transformer(x, attn_mask=attn)  # [L,C,W]\n",
        "            x = x.permute(1, 0, 2)                         # [C,L,W]\n",
        "            x = self.text_tower.ln_final(x).to(text_dtype)      # [C,L,W]\n",
        "\n",
        "        # Pool at EOT (shift by n_ctx), clamp in case of truncation\n",
        "        eot_indices = (eot_indices + self.ctx.size(0)).clamp(max=L-1)\n",
        "        text_emb = x[torch.arange(B, device=eot_indices.device), eot_indices] @ self.text_tower.text_projection  # [C,D]\n",
        "\n",
        "        # Normalize\n",
        "        text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
        "        return text_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NQsIUL3VioVZ",
      "metadata": {
        "id": "NQsIUL3VioVZ"
      },
      "outputs": [],
      "source": [
        "class CoOpClip(nn.Module):\n",
        "    def __init__(self, clip, coop_prompt):\n",
        "        super().__init__()\n",
        "        self.clip = clip\n",
        "        self.coop_prompt = coop_prompt\n",
        "\n",
        "        self.clip.requires_grad_(False)\n",
        "        self.coop_prompt.requires_grad_(True)\n",
        "\n",
        "    def train(self, mode: bool=True):\n",
        "        super().train(mode)\n",
        "        if mode:\n",
        "          self.clip.eval()\n",
        "          self.coop_prompt.train()\n",
        "        else:\n",
        "          self.clip.eval()\n",
        "          self.coop_prompt.eval()\n",
        "\n",
        "    def forward_text(self, texts):\n",
        "        return self.coop_prompt(texts)\n",
        "\n",
        "    def forward_image(self, images):\n",
        "        image_features = self.clip.encode_image(images)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip.encode_image(images)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = self.coop_prompt(texts)\n",
        "        logits = 100.0 * image_features @ text_features.T\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91503efb",
      "metadata": {
        "id": "91503efb"
      },
      "outputs": [],
      "source": [
        "def ret_accuracy(logits): # Accuracy for retrieval (from text to image only!)!\n",
        "    preds = logits.argmax(dim=0) # Per text prediction\n",
        "    labels = torch.arange(0, logits.size(0), device=logits.device)\n",
        "    correct = (preds == labels).float().mean().item()\n",
        "    return correct\n",
        "\n",
        "def cls_accuracy(logits, labels): # Accuracy for classification\n",
        "    preds = logits.argmax(dim=1) # Per image prediction\n",
        "    correct = (preds == labels).float().mean().item()\n",
        "    return correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7I1GVg7jk1Rb",
      "metadata": {
        "id": "7I1GVg7jk1Rb"
      },
      "outputs": [],
      "source": [
        "class SimetricalCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, logits):\n",
        "        bs = logits.size(0)\n",
        "        labels = torch.arange(bs, device=logits.device)\n",
        "        loss_1 = self.criterion(logits, labels)\n",
        "        loss_2 = self.criterion(logits.T, labels)\n",
        "        return (loss_1 + loss_2) / 2.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_retrieval(model, test_loader, loss_fn, device, desc=\"eval\", pbar=True):\n",
        "  model.eval()\n",
        "  loss_sum, correct, count = 0.0, 0, 0\n",
        "  iterator = tqdm(test_loader, desc=desc, leave=False) if pbar else test_loader\n",
        "  all_image_features = []\n",
        "  all_text_features = []\n",
        "  for batch in iterator:\n",
        "    images, description_ids = batch[\"pixel_values\"], batch[\"description_ids\"]\n",
        "    images = images.to(device, non_blocking=True)\n",
        "    description_ids = description_ids.to(device, non_blocking=True)\n",
        "\n",
        "    image_features = model.forward_image(images)\n",
        "    text_features = model.forward_text(description_ids)\n",
        "\n",
        "    all_image_features.append(image_features)\n",
        "    all_text_features.append(text_features)\n",
        "\n",
        "  # For the true accuracy, it is needed to retrieve the text from all the images\n",
        "  all_image_features = torch.cat(all_image_features, dim=0)\n",
        "  all_text_features = torch.cat(all_text_features, dim=0)\n",
        "\n",
        "  logits = all_image_features @ all_text_features.T\n",
        "\n",
        "  loss = loss_fn(logits)\n",
        "  acc = ret_accuracy(logits)\n",
        "  return loss.item(), acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_classifier(model, classnames, test_loader, loss_fn, device, desc=\"eval\", pbar=True):\n",
        "  model.eval()\n",
        "  loss_sum, correct, count = 0.0, 0, 0\n",
        "  iterator = tqdm(test_loader, desc=desc, leave=False) if pbar else test_loader\n",
        "  #all_image_features = []\n",
        "  class_token_ids = tokenizer(classnames).to(device)\n",
        "  #print(class_token_ids)\n",
        "  #return None, None\n",
        "\n",
        "  for images, labels in iterator:\n",
        "    images = images.to(device, non_blocking=True)\n",
        "    labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "    logits = model.forward(images, class_token_ids)\n",
        "\n",
        "    loss = loss_fn(logits, labels)\n",
        "    preds = logits.argmax(dim=1)\n",
        "\n",
        "    bs = labels.size(0)\n",
        "    loss_sum += loss.item()\n",
        "    correct += (preds == labels).sum().item()\n",
        "    count += bs\n",
        "\n",
        "    if pbar:\n",
        "      acc = (correct / max(1, count)) * 100.0\n",
        "      iterator.set_postfix(loss=loss.item(), acc=f\"{acc:.2f}%\")\n",
        "\n",
        "  avg_loss = loss_sum / max(1, count)\n",
        "  avg_acc = correct / max(1, count)\n",
        "  return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "def train(\n",
        "    model, train_loader, val_loader, device=\"cpu\",\n",
        "    epochs=5, batch_size=256, lr=5e-3, log_train_interval=25, log_val_interval=None\n",
        "):\n",
        "    model.to(device)\n",
        "\n",
        "    print(model.coop_prompt.parameters())\n",
        "    optimizer = torch.optim.AdamW([model.coop_prompt.ctx], lr=lr)\n",
        "    loss_fn = SimetricalCrossEntropyLoss()\n",
        "\n",
        "    t = 0 # current_time_step\n",
        "    # Todo add batches per epoch\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_time': [], 'val_time': []\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss, val_acc = evaluate_retrieval(\n",
        "            model, val_loader, loss_fn=loss_fn, device=device, desc=\"valid\", pbar=True\n",
        "        )\n",
        "        history['val_time'].append(t)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        tqdm.write(\n",
        "            f\"Before training: \"\n",
        "            f\"val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        iterator = tqdm(\n",
        "            train_loader,\n",
        "            desc=f\"Epoch {epoch}/{epochs} | train\",\n",
        "            leave=False,\n",
        "            dynamic_ncols=True,\n",
        "            mininterval=0.5,\n",
        "        )\n",
        "\n",
        "        for step, batch in enumerate(iterator, start=1):\n",
        "            images, description_ids = batch[\"pixel_values\"], batch[\"description_ids\"]\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            description_ids = description_ids.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(images, description_ids)\n",
        "            loss = loss_fn(logits)\n",
        "            acc = accuracy(logits)\n",
        "\n",
        "            history['train_time'].append(t)\n",
        "            history['train_loss'].append(loss.item())\n",
        "            history['train_acc'].append(acc)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            t += 1\n",
        "\n",
        "            if log_train_interval and t % log_train_interval == 0:\n",
        "                iterator.set_postfix_str(\n",
        "                    f\"loss={loss.item():.4f}  acc={acc*100:.2f}  val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\"\n",
        "                )\n",
        "\n",
        "            if log_val_interval and t % log_val_interval == 0:\n",
        "                val_loss, val_acc = evaluate_retrieval(\n",
        "                     model, val_loader, loss_fn=loss_fn, device=device, desc=\"valid\", pbar=False\n",
        "                )\n",
        "                history['val_loss'].append(val_loss)\n",
        "                history['val_acc'].append(val_acc)\n",
        "                history['val_time'].append(t)\n",
        "                iterator.set_postfix_str(\n",
        "                    f\"loss={loss.item():.4f}  acc={acc*100:.2f}  val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\"\n",
        "                )\n",
        "\n",
        "        avg_train_loss = np.mean(history['train_loss'][-len(train_loader):])\n",
        "        avg_train_acc = np.mean(history['train_acc'][-len(train_loader):])\n",
        "        avg_val_loss = history['val_loss'][-1]\n",
        "        avg_val_acc = history['val_acc'][-1]\n",
        "\n",
        "        tqdm.write(\n",
        "            f\"Epoch {epoch:03d}: \"\n",
        "            f\"train_loss={avg_train_loss:.4f}  train_acc={avg_train_acc*100:.2f}%  \"\n",
        "            f\"val_loss={avg_val_loss:.4f}  val_acc={avg_val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infinite_loader(dataloader):\n",
        "    while True:\n",
        "        for batch in dataloader:\n",
        "            yield batch"
      ],
      "metadata": {
        "id": "MXtUrCySdTy-"
      },
      "id": "MXtUrCySdTy-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "529BX7eQh2X5"
      },
      "id": "529BX7eQh2X5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def train_multitask(\n",
        "    model, ret_train_loader, ret_val_loader, classnames, cls_train_loader, cls_val_loader, device=\"cpu\",\n",
        "    epochs=5, batch_size=256, lr=5e-3, log_train_interval=25, log_val_interval=None,\n",
        "):\n",
        "    class_token_ids = tokenizer(classnames).to(device)\n",
        "    inf_cls_train_loader = infinite_loader(cls_train_loader)\n",
        "    #inf_cls_val_loader = infinite_loader(cls_val_loader)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    print(model.coop_prompt.parameters())\n",
        "    optimizer = torch.optim.AdamW([model.coop_prompt.ctx], lr=lr)\n",
        "    ret_loss_fn = SimetricalCrossEntropyLoss()\n",
        "    cls_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    t = 0 # current_time_step\n",
        "    # Todo add batches per epoch\n",
        "\n",
        "    per_task_empty_history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': [],\n",
        "        'train_time': [], 'val_time': []\n",
        "    }\n",
        "\n",
        "    history = {\n",
        "        \"ret\": deepcopy(per_task_empty_history),\n",
        "        \"cls\": deepcopy(per_task_empty_history),\n",
        "    }\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        task = \"ret\"\n",
        "        ret_val_loss, ret_val_acc = evaluate_retrieval(\n",
        "            model, ret_val_loader, loss_fn=ret_loss_fn, device=device, desc=\"valid\", pbar=True\n",
        "        )\n",
        "        history[task]['val_time'].append(t)\n",
        "        history[task]['val_loss'].append(ret_val_loss)\n",
        "        history[task]['val_acc'].append(ret_val_acc)\n",
        "\n",
        "        task = \"cls\"\n",
        "        cls_val_loss, cls_val_acc = evaluate_classifier(\n",
        "            model, classnames, cls_val_loader, loss_fn=cls_loss_fn, device=device, desc=\"valid\", pbar=True\n",
        "        )\n",
        "        history[task]['val_time'].append(t)\n",
        "        history[task]['val_loss'].append(cls_val_loss)\n",
        "        history[task]['val_acc'].append(cls_val_acc)\n",
        "\n",
        "\n",
        "        tqdm.write(\n",
        "            f\"Before training: \"\n",
        "            f\"\"\"ret_val_loss={ret_val_loss:.4f}  ret_val_acc={ret_val_acc*100:.2f}%\\n\n",
        "                cls_val_loss={cls_val_loss:.4f}  cls_val_acc={cls_val_acc*100:.2f}%\"\"\"\n",
        "        )\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "\n",
        "        iterator = tqdm(\n",
        "            ret_train_loader,\n",
        "            desc=f\"Epoch {epoch}/{epochs} | train\",\n",
        "            leave=False,\n",
        "            dynamic_ncols=True,\n",
        "            mininterval=0.5,\n",
        "        )\n",
        "\n",
        "        for step, batch in enumerate(iterator, start=1):\n",
        "            images, description_ids = batch[\"pixel_values\"], batch[\"description_ids\"]\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            description_ids = description_ids.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(images, description_ids)\n",
        "            ret_loss = ret_loss_fn(logits)\n",
        "            ret_acc = ret_accuracy(logits)\n",
        "\n",
        "            task = \"ret\"\n",
        "            history[task]['train_time'].append(t)\n",
        "            history[task]['train_loss'].append(ret_loss.item())\n",
        "            history[task]['train_acc'].append(ret_acc)\n",
        "\n",
        "            images, labels = next(inf_cls_train_loader)\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(images, class_token_ids)\n",
        "            cls_loss = cls_loss_fn(logits, labels)\n",
        "            cls_acc = cls_accuracy(logits, labels)\n",
        "\n",
        "            task = \"cls\"\n",
        "            history[task]['train_time'].append(t)\n",
        "            history[task]['train_loss'].append(cls_loss.item())\n",
        "            history[task]['train_acc'].append(cls_acc)\n",
        "\n",
        "            # Composed loss\n",
        "            loss = ret_loss + 5 * cls_loss\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            t += 1\n",
        "\n",
        "            if t == 1:\n",
        "                pprint(\"DEBUG::\")\n",
        "                pprint(history)\n",
        "\n",
        "            if log_train_interval and t % log_train_interval == 0:\n",
        "                iterator.set_postfix_str(\n",
        "                    f\"\"\"ret_loss={ret_loss.item():.4f}  ret_acc={ret_acc*100:.2f}  val_loss={ret_val_loss:.4f}  val_acc={ret_val_acc*100:.2f}%\n",
        "                    cls_loss={cls_loss.item():.4f}  cls_acc={cls_acc*100:.2f}  val_loss={cls_val_loss:.4f}  val_acc={cls_val_acc*100:.2f}\n",
        "                    \"\"\"\n",
        "                )\n",
        "\n",
        "            if log_val_interval and t % log_val_interval == 0:\n",
        "                ret_val_loss, ret_val_acc = evaluate_retrieval(\n",
        "                     model, ret_val_loader, loss_fn=ret_loss_fn, device=device, desc=\"valid\", pbar=False\n",
        "                )\n",
        "\n",
        "                task = \"ret\"\n",
        "                history[task]['val_loss'].append(ret_val_loss)\n",
        "                history[task]['val_acc'].append(ret_val_acc)\n",
        "                history[task]['val_time'].append(t)\n",
        "                iterator.set_postfix_str(\n",
        "                    f\"ret_loss={ret_loss.item():.4f}  ret_acc={ret_acc*100:.2f}  ret_val_loss={ret_val_loss:.4f}  ret_val_acc={ret_val_acc*100:.2f}%\"\n",
        "                )\n",
        "\n",
        "        task = \"ret\"\n",
        "        avg_train_loss = np.mean(history[task]['train_loss'][-len(ret_train_loader):])\n",
        "        avg_train_acc = np.mean(history[task]['train_acc'][-len(ret_train_loader):])\n",
        "        avg_val_loss = history[task]['val_loss'][-1]\n",
        "        avg_val_acc = history[task]['val_acc'][-1]\n",
        "\n",
        "        tqdm.write(\n",
        "            f\"Epoch {epoch:03d}: \"\n",
        "            f\"train_loss={avg_train_loss:.4f}  train_acc={avg_train_acc*100:.2f}%  \"\n",
        "            f\"val_loss={avg_val_loss:.4f}  val_acc={avg_val_acc*100:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "Xgjw9WXdPaCl"
      },
      "id": "Xgjw9WXdPaCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9YJLsTcXk7Za",
      "metadata": {
        "id": "9YJLsTcXk7Za"
      },
      "outputs": [],
      "source": [
        "coop_prompt = CoOpPrompt(\n",
        "    clip=clip_model,\n",
        "    tokenizer=tokenizer,\n",
        "    n_ctx=cfg.prompt_len,\n",
        ")\n",
        "\n",
        "coopclip_model = CoOpClip(\n",
        "    clip=clip_model,\n",
        "    coop_prompt=coop_prompt,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train only retrieval\n",
        "#history = train(\n",
        "#    model=coopclip_model,\n",
        "#    train_loader=train_loader,\n",
        "#    val_loader=val_loader,\n",
        "#    device=device,\n",
        "#    epochs=cfg.max_epochs,\n",
        "#    batch_size=cfg.batch_size,\n",
        "#    lr=cfg.lr_prompt,\n",
        "#    log_train_interval=10,\n",
        "#    log_val_interval=30\n",
        "#)"
      ],
      "metadata": {
        "id": "moD4kD3biLWA"
      },
      "id": "moD4kD3biLWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dFteCMDNRxb5",
      "metadata": {
        "id": "dFteCMDNRxb5"
      },
      "outputs": [],
      "source": [
        "loss, acc = evaluate_retrieval(\n",
        "    model=coopclip_model,\n",
        "    test_loader=val_loader,\n",
        "    device=device,\n",
        "    loss_fn=SimetricalCrossEntropyLoss(),\n",
        "    desc=\"Testing trained CoOpClip on full val\",\n",
        "    pbar=True\n",
        ")\n",
        "print(f\"Accuracy after training: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_classifier(\n",
        "    model=coopclip_model,\n",
        "    classnames=classnames,\n",
        "    test_loader=cls_val_loader,\n",
        "    device=device,\n",
        "    loss_fn=nn.CrossEntropyLoss(),\n",
        "    desc=\"Testing trained CoOpClip on full val\",\n",
        "    pbar=True\n",
        ")\n",
        "\n",
        "print(f\"Accuracy for classification: {acc}\")"
      ],
      "metadata": {
        "id": "iokambblRyej"
      },
      "id": "iokambblRyej",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_multitask(\n",
        "    model=coopclip_model,\n",
        "    ret_train_loader=train_loader,\n",
        "    ret_val_loader=val_loader,\n",
        "    classnames=classnames,\n",
        "    cls_train_loader=cls_train_loader,\n",
        "    cls_val_loader=cls_val_loader,\n",
        "    device=device,\n",
        "    epochs=cfg.max_epochs,\n",
        "    batch_size=cfg.batch_size,\n",
        "    lr=cfg.lr_prompt,\n",
        "    log_train_interval=cfg.log_train_interval,\n",
        "    log_val_interval=cfg.log_val_interval\n",
        ")"
      ],
      "metadata": {
        "id": "WMw_bkWZYXky"
      },
      "id": "WMw_bkWZYXky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cGaO-eNxsFSk",
      "metadata": {
        "id": "cGaO-eNxsFSk"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), constrained_layout=True)\n",
        "    # Delete me\n",
        "    #val_losses = []\n",
        "    #for val_loss in history['val_loss']:\n",
        "    #  val_losses.append(val_loss.cpu().item())\n",
        "    #history['val_loss'] = val_losses\n",
        "    #del history['val_loss']\n",
        "\n",
        "    # --- Loss (left) ---\n",
        "    ax = axes[0]\n",
        "    ax.plot(history['train_time'], history['train_loss'], label='train', linewidth=1.5)\n",
        "    if 'val_time' in history and 'val_loss' in history and len(history['val_time']) and len(history['val_loss']):\n",
        "        ax.plot(history['val_time'], history['val_loss'].cpu(), label='val', linewidth=1.2, marker='o', markersize=3)\n",
        "    ax.set_xlabel('Time'); ax.set_ylabel('Loss'); ax.set_title('Loss'); ax.legend(); ax.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    # --- Accuracy (right) ---\n",
        "    ax = axes[1]\n",
        "    if 'train_acc' in history and len(history['train_acc']):\n",
        "        ax.plot(history['train_time'], history['train_acc'], label='train', linewidth=1.5)\n",
        "    if 'val_time' in history and 'val_acc' in history and len(history['val_time']) and len(history['val_acc']):\n",
        "        ax.plot(history['val_time'], history['val_acc'], label='val', linewidth=1.2, marker='o', markersize=3)\n",
        "    ax.set_xlabel('Time'); ax.set_ylabel('Accuracy'); ax.set_title('Accuracy'); ax.legend(); ax.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(history)"
      ],
      "metadata": {
        "id": "5WMYqi1cryfz"
      },
      "id": "5WMYqi1cryfz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GcBfYSsDtQX3",
      "metadata": {
        "id": "GcBfYSsDtQX3"
      },
      "outputs": [],
      "source": [
        "plot_history(history[\"ret\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history[\"cls\"])"
      ],
      "metadata": {
        "id": "RgXfXVeqr2YP"
      },
      "id": "RgXfXVeqr2YP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2tDOCkz7F8t",
      "metadata": {
        "id": "W2tDOCkz7F8t"
      },
      "outputs": [],
      "source": [
        "import torch, hashlib, json\n",
        "\n",
        "def save_coop_prompt(coop_prompt: nn.Module, path: str, *,\n",
        "                     model_name: str, pretrained: str, n_ctx: int):\n",
        "    # minimal payload: the ctx tensor + metadata to sanity-check at load time\n",
        "    payload = {\n",
        "        \"ctx\": coop_prompt.ctx.detach().cpu(),  # device-agnostic\n",
        "        \"meta\": {\n",
        "            \"model_name\": model_name,\n",
        "            \"pretrained\": pretrained,\n",
        "            \"n_ctx\": int(n_ctx),\n",
        "        }\n",
        "    }\n",
        "    torch.save(payload, path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y_xUQTHV7f69",
      "metadata": {
        "id": "y_xUQTHV7f69"
      },
      "outputs": [],
      "source": [
        "save_coop_prompt(coopclip_model.coop_prompt, cfg.coop_prompt_save_path,\n",
        "                 model_name=cfg.model_name, pretrained=cfg.pretrained,\n",
        "                 n_ctx=cfg.prompt_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1HgY0IXdOsDx",
      "metadata": {
        "id": "1HgY0IXdOsDx"
      },
      "outputs": [],
      "source": [
        "def load_coop_prompt(clip: nn.Module, tokenizer, path: str):\n",
        "  payload = torch.load(path)\n",
        "  ctx = payload[\"ctx\"]\n",
        "  n_ctx = payload[\"meta\"][\"n_ctx\"]\n",
        "  model_name = payload[\"meta\"][\"model_name\"]\n",
        "  if cfg.model_name != model_name:\n",
        "    raise ValueError(f\"Model from path: '{model_name}' doesn't match the cfg.modelname: '{cfg.model_name}'\")\n",
        "  coop_prompt = CoOpPrompt(clip, tokenizer, n_ctx)\n",
        "  with torch.no_grad():\n",
        "    coop_prompt.ctx.copy_(ctx)\n",
        "\n",
        "  return coop_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BEeCDinOQ-Q8",
      "metadata": {
        "id": "BEeCDinOQ-Q8"
      },
      "outputs": [],
      "source": [
        "loaded_prompt = load_coop_prompt(clip_model, tokenizer, cfg.coop_prompt_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YMeXWlCLRMZh",
      "metadata": {
        "id": "YMeXWlCLRMZh"
      },
      "outputs": [],
      "source": [
        "loaded_coopclip_model = CoOpClip(\n",
        "    clip=clip_model,\n",
        "    coop_prompt=loaded_prompt,\n",
        ").to(device)\n",
        "\n",
        "loss, acc = evaluate_retrieval(\n",
        "    model=loaded_coopclip_model,\n",
        "    test_loader=val_loader,\n",
        "    device=device,\n",
        "    loss_fn=SimetricalCrossEntropyLoss(),\n",
        "    desc=\"Testing CoOpClip on full val\",\n",
        "    pbar=True\n",
        ")\n",
        "print(f\"Accuracy for loaded model: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "@torch.no_grad()\n",
        "def precompute_image_features(model, loader):\n",
        "  loaded_coopclip_model.eval()\n",
        "  image_features = []\n",
        "  for batch in loader:\n",
        "    images = batch[\"pixel_values\"]\n",
        "    images = images.to(device, non_blocking=True)\n",
        "    image_features.append(loaded_coopclip_model.forward_image(images).detach().cpu())\n",
        "  image_features = torch.cat(image_features, dim=0)\n",
        "  return image_features\n",
        "\n",
        "val_image_features = precompute_image_features(loaded_coopclip_model, val_loader)\n",
        "#plt.imshow(best_image[0].permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "2YnXrPJZ_pbc"
      },
      "id": "2YnXrPJZ_pbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def retrieve(model, text, image_features, top_k=1):\n",
        "    image_features = image_features.to(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    text_features = model.forward_text(tokenizer(text).to(device))\n",
        "\n",
        "    logits = image_features @ text_features.T\n",
        "    logits = logits.squeeze(1)\n",
        "    preds = (-logits).argsort(dim=0)[:top_k].detach().cpu()\n",
        "    scores = logits[preds].detach().cpu()\n",
        "\n",
        "    return preds, scores\n",
        "\n",
        "top_k = 3\n",
        "\n",
        "preds, scores = retrieve(loaded_coopclip_model, \"Ocean\", val_image_features, top_k=top_k)\n",
        "\n",
        "for i in range(top_k):\n",
        "    image = torch.as_tensor(val_loader.dataset[preds[i].item()]['pixel_values'])\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    plt.title(f\"Score: {scores[i]}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5ALAfonLGzOk"
      },
      "id": "5ALAfonLGzOk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean GPU memory, cuz for whatever reason, images stick on the gpu even after training"
      ],
      "metadata": {
        "id": "_DY1rgiDZ2_x"
      },
      "id": "_DY1rgiDZ2_x"
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
      ],
      "metadata": {
        "id": "axAF08tlAXak"
      },
      "id": "axAF08tlAXak",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math\n",
        "\n",
        "def list_cuda_tensors(namespace=None):\n",
        "    if namespace is None:\n",
        "        namespace = globals()\n",
        "    rows = []\n",
        "    for name, obj in list(namespace.items()):\n",
        "        if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
        "            mb = obj.numel() * obj.element_size() / (1024**2)\n",
        "            rows.append((name, tuple(obj.shape), str(obj.dtype), str(obj.device), f\"{mb:.1f} MB\"))\n",
        "    rows.sort(key=lambda r: float(r[-1].split()[0]), reverse=True)\n",
        "    for r in rows:\n",
        "        print(r)\n",
        "list_cuda_tensors()"
      ],
      "metadata": {
        "id": "h4CtQPzBFFrq"
      },
      "id": "h4CtQPzBFFrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "# drop any live refs\n",
        "images = image_features = logits = pred = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "LVG9PSVjGALI"
      },
      "id": "LVG9PSVjGALI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}