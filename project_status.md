- Yes, we have acquired the Flickr30k dataset along with CIFAR10, Flower102, and EuroSAT.
- Yes, we set up two neural networks: one for classification and one for retrieval, using MobileCLIP2 with a context-optimizable prompt (CoOp).
- Yes, we trained prompts for multiple datasets to improve accuracy.
- Yes, we ve identified problems, especially: During early development, there was exactly 0 gradient flow from the logits back to the optimizable prompt. Thankfully, using the method from the course to inspect the gradient at any specific layer helped us, showing that there was a problem at the transformer level (We hardly found out that we were not using the correct mask). Aditionally, training was time-consuming across datasets, so we implemented code to save only the trained context (very small) and reload it later - two versions for classification and retrieval. Another problem was that the code was kinda messy, and it was hard to experiment. With the new structure, the prompts can be easily swaped/retrain, without reloading the main model, which should make our experiments easier.
- Next, we plan to train a prompt for the image encoder and fine-tune the upper layers for improved jointÂ performance.  We also plan to train a prompt that balances retrieval and classification accuracy using a combined loss for hopefully better generalization.
- NOTE: Unfortunately, we currently don't have the experiments results in a centralized place, as we fastly developed, but we plan on rerunning the most important experiments with the new code structure and save them in a csv, and plot the results nicely. (accuracy improvements, training times, bad examples). Inspecting these, many more improvement ideas may come up.